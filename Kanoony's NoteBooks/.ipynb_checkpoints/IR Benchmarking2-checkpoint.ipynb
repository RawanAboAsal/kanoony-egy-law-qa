{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13581d2c",
   "metadata": {},
   "source": [
    "## Information Retrieval Benchmarking on Legal Texts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4410e8d1",
   "metadata": {},
   "source": [
    "### Evaluation Pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a6baec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import ndcg_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from rank_bm25 import BM25Okapi\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from openai import AzureOpenAI\n",
    "import tiktoken\n",
    "import google.generativeai as genai\n",
    "from mistralai import Mistral\n",
    "import random\n",
    "from sentence_transformers import InputExample\n",
    "import json\n",
    "import pandas as pd\n",
    "import random\n",
    "from sentence_transformers import InputExample, SentenceTransformer\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from farasa.segmenter import FarasaSegmenter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d043bbcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_law_texts(law_texts: list) -> list:\n",
    "    return [embed_query(text) for text in law_texts]\n",
    "\n",
    "def build_bm25_index(law_texts: list):\n",
    "    tokenized_texts = [text.split() for text in law_texts]\n",
    "    bm25 = BM25Okapi(tokenized_texts)\n",
    "    return bm25\n",
    "\n",
    "def bm25_search(query: str, bm25: BM25Okapi, k=100):\n",
    "    tokenized_query = query.split()\n",
    "    scores = bm25.get_scores(tokenized_query)\n",
    "    ranked_indices = np.argsort(scores)[::-1]\n",
    "    return ranked_indices[:k], scores\n",
    "\n",
    "def semantic_search(query_embedding: np.ndarray, law_text_embeddings: list, top_k=100):\n",
    "    similarities = cosine_similarity([query_embedding], law_text_embeddings)[0]\n",
    "    ranked_indices = np.argsort(similarities)[::-1]\n",
    "    return ranked_indices[:top_k], similarities\n",
    "\n",
    "def rrf_fusion(bm25_indices, semantic_indices, k=60):\n",
    "    fused_scores = defaultdict(float)\n",
    "\n",
    "    for rank, idx in enumerate(bm25_indices):\n",
    "        fused_scores[idx] += 1 / (k + rank)\n",
    "\n",
    "    for rank, idx in enumerate(semantic_indices):\n",
    "        fused_scores[idx] += 1 / (k + rank)\n",
    "\n",
    "    fused_sorted = sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    fused_indices = [idx for idx, _ in fused_sorted]\n",
    "    return fused_indices\n",
    "\n",
    "def average_precision(relevant_docs, retrieved_docs):\n",
    "    relevant = set(relevant_docs)\n",
    "    retrieved = retrieved_docs\n",
    "\n",
    "    precisions = []\n",
    "    num_relevant = 0\n",
    "\n",
    "    for i, doc in enumerate(retrieved):\n",
    "        if doc in relevant:\n",
    "            num_relevant += 1\n",
    "            precisions.append(num_relevant / (i + 1))\n",
    "\n",
    "    if not precisions:\n",
    "        return 0.0\n",
    "\n",
    "    return sum(precisions) / len(relevant)\n",
    "\n",
    "def calculate_ndcg(relevant_docs, retrieved_docs, k):\n",
    "    relevance_scores = np.zeros(len(retrieved_docs))\n",
    "    for i, doc in enumerate(retrieved_docs[:k]):\n",
    "        if doc in relevant_docs:\n",
    "            relevance_scores[i] = 1\n",
    "\n",
    "    if sum(relevance_scores) == 0:\n",
    "        return 0.0\n",
    "\n",
    "    dcg = 0.0\n",
    "    for i, rel in enumerate(relevance_scores[:k]):\n",
    "        dcg += rel / np.log2(i + 2)  \n",
    "\n",
    "    ideal_relevance = np.sort(relevance_scores)[::-1]\n",
    "    idcg = 0.0\n",
    "    for i, rel in enumerate(ideal_relevance[:k]):\n",
    "        idcg += rel / np.log2(i + 2)\n",
    "\n",
    "    return dcg / idcg if idcg > 0 else 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58bf2473",
   "metadata": {},
   "source": [
    "### Ada 3 small "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2065d1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = \"https://eslsca-openai.openai.azure.com/\"\n",
    "deployment = \"text-embedding-3-small\"  \n",
    "subscription_key = \"0d368117945a4cb8a0f5b282dd192340\"\n",
    "\n",
    "client = AzureOpenAI(\n",
    "    azure_endpoint=endpoint,\n",
    "    api_key=subscription_key,\n",
    "    api_version=\"2024-05-01-preview\",\n",
    ")\n",
    "\n",
    "tokenizer = tiktoken.encoding_for_model(\"text-embedding-3-small\")\n",
    "MAX_TOKENS = 8191  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a6a6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text(text):\n",
    "    tokens = tokenizer.encode(text)\n",
    "    chunked_texts = []\n",
    "    for i in range(0, len(tokens), MAX_TOKENS):\n",
    "        chunk_tokens = tokens[i:i + MAX_TOKENS]\n",
    "        chunked_texts.append(tokenizer.decode(chunk_tokens))\n",
    "    return chunked_texts\n",
    "\n",
    "def generate_embeddings(text, model=\"text-embedding-3-small\"):\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        raise ValueError(\"Invalid input: input must be a non-empty string.\")\n",
    "\n",
    "    if len(tokenizer.encode(text)) > MAX_TOKENS:\n",
    "        text_chunks = split_text(text)\n",
    "    else:\n",
    "        text_chunks = [text]\n",
    "\n",
    "    embeddings = []\n",
    "    for chunk in text_chunks:\n",
    "        response = client.embeddings.create(input=[chunk], model=model)\n",
    "        embeddings.append(response.data[0].embedding)\n",
    "\n",
    "    final_embedding = np.mean(embeddings, axis=0)  \n",
    "    return final_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec97bf27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_law_text_from_json(json_file):\n",
    "    with open(json_file, \"r\", encoding=\"utf-8\") as file:\n",
    "        laws_data = json.load(file)\n",
    "    law_texts = [law[\"Law_Text\"] for law in laws_data]\n",
    "    return law_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3818aac1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1rb34KJptHT2",
    "outputId": "af5b8dae-d959-449d-d6cb-9c1893ba747c"
   },
   "outputs": [
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings:  13%|█▎        | 6297/49521 [15:53<56:35, 12.73it/s]  "
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Skipped 6293] Reason: Invalid input: input must be a non-empty string.\n",
      "[Skipped 6295] Reason: Invalid input: input must be a non-empty string.\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings:  13%|█▎        | 6307/49521 [15:54<1:04:52, 11.10it/s]"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Skipped 6305] Reason: Invalid input: input must be a non-empty string.\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings:  14%|█▍        | 7072/49521 [17:18<58:13, 12.15it/s]"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Skipped 7069] Reason: Invalid input: input must be a non-empty string.\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings:  14%|█▍        | 7091/49521 [17:20<59:39, 11.86it/s]  "
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Skipped 7089] Reason: Invalid input: input must be a non-empty string.\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings:  14%|█▍        | 7120/49521 [17:23<1:01:08, 11.56it/s]"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Skipped 7119] Reason: Invalid input: input must be a non-empty string.\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings:  18%|█▊        | 8904/49521 [21:23<1:04:52, 10.43it/s]"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Skipped 8902] Reason: Invalid input: input must be a non-empty string.\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings:  34%|███▍      | 17030/49521 [45:37<49:02, 11.04it/s]  "
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Skipped 17027] Reason: Invalid input: input must be a non-empty string.\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings:  37%|███▋      | 18311/49521 [48:40<47:27, 10.96it/s]"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Skipped 18309] Reason: Invalid input: input must be a non-empty string.\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings:  41%|████▏     | 20494/49521 [54:59<42:43, 11.32it/s]"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Skipped 20492] Reason: Invalid input: input must be a non-empty string.\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings:  43%|████▎     | 21109/49521 [57:03<39:32, 11.98it/s]"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Skipped 21107] Reason: Invalid input: input must be a non-empty string.\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings:  65%|██████▌   | 32196/49521 [1:33:04<27:29, 10.50it/s]"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Skipped 32194] Reason: Invalid input: input must be a non-empty string.\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings:  68%|██████▊   | 33554/49521 [1:37:03<22:06, 12.04it/s]"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Skipped 33552] Reason: Invalid input: input must be a non-empty string.\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings:  68%|██████▊   | 33866/49521 [1:37:36<24:13, 10.77it/s]"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Skipped 33864] Reason: Invalid input: input must be a non-empty string.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings:  79%|███████▊  | 38931/49521 [1:50:24<16:08, 10.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Skipped 38929] Reason: Invalid input: input must be a non-empty string.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings:  79%|███████▉  | 39001/49521 [1:50:31<16:36, 10.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Skipped 38999] Reason: Invalid input: input must be a non-empty string.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings:  94%|█████████▎| 46409/49521 [2:11:22<04:22, 11.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Skipped 46407] Reason: Invalid input: input must be a non-empty string.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings:  99%|█████████▉| 49058/49521 [2:16:40<00:38, 11.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Skipped 49056] Reason: Invalid input: input must be a non-empty string.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings:  99%|█████████▉| 49075/49521 [2:16:41<00:39, 11.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Skipped 49072] Reason: Invalid input: input must be a non-empty string.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings: 100%|██████████| 49521/49521 [2:17:27<00:00,  6.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings saved to 'law_embeddings.npy'. Shape: (49502, 1536)\n",
      "Skipped 19 entries.\n"
     ]
    }
   ],
   "source": [
    "json_file = \"/content/drive/MyDrive/articles.json\"  \n",
    "law_texts = extract_law_text_from_json(json_file)\n",
    "law_embeddings = []\n",
    "skipped = []\n",
    "\n",
    "for idx, text in tqdm(enumerate(law_texts), total=len(law_texts), desc=\"Generating embeddings\"):\n",
    "    try:\n",
    "        embedding = generate_embeddings(text)\n",
    "        law_embeddings.append(embedding)\n",
    "    except Exception as e:\n",
    "        print(f\"[Skipped {idx}] Reason: {e}\")\n",
    "        skipped.append(idx)\n",
    "\n",
    "law_embeddings = np.array(law_embeddings)\n",
    "np.save(\"/content/drive/MyDrive/law_embeddings.npy\", law_embeddings)\n",
    "\n",
    "print(f\"Embeddings saved to 'law_embeddings.npy'. Shape: {law_embeddings.shape}\")\n",
    "print(f\"Skipped {len(skipped)} entries.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7584d880",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XXP6oZJVJLJ1",
    "outputId": "2f6dcf08-2294-480c-a687-3dd21a2a24e9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.05179457, -0.01040705,  0.02915129, ..., -0.005088  ,\n",
       "         0.01804145,  0.00913625],\n",
       "       [-0.01238826, -0.00120428,  0.03797311, ..., -0.00015582,\n",
       "        -0.01076218,  0.0131496 ],\n",
       "       [-0.00831607, -0.02329179,  0.04631175, ..., -0.03018928,\n",
       "        -0.02894909, -0.01863683],\n",
       "       ...,\n",
       "       [-0.01761828,  0.01699308,  0.06670143, ..., -0.02312207,\n",
       "         0.00127346,  0.01466653],\n",
       "       [ 0.00454035, -0.01636438,  0.08503358, ...,  0.00263818,\n",
       "        -0.03769924,  0.01017994],\n",
       "       [ 0.02106914, -0.032288  ,  0.06716913, ..., -0.00746424,\n",
       "        -0.03560144, -0.00385592]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "law_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23df172c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WOXOHskM61TV",
    "outputId": "046d393b-b3e8-4a35-dc20-fa6e728be140"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Final Scores ===\n",
      "BM25 -> nDCG@10: 0.7556, nDCG@100: 0.7731, MAP: 0.7185\n",
      "Semantic -> nDCG@10: 0.7920, nDCG@100: 0.8058, MAP: 0.7483\n",
      "Hybrid -> nDCG@10: 0.8386, nDCG@100: 0.8500, MAP: 0.8053\n"
     ]
    }
   ],
   "source": [
    "client = AzureOpenAI(\n",
    "    azure_endpoint=\"https://eslsca-openai.openai.azure.com/\",\n",
    "    api_key=\"0d368117945a4cb8a0f5b282dd192340\",\n",
    "    api_version=\"2024-05-01-preview\",\n",
    ")\n",
    "\n",
    "tokenizer = tiktoken.encoding_for_model(\"text-embedding-3-small\")\n",
    "MAX_TOKENS = 8191\n",
    "\n",
    "def embed_query(text: str, model=\"text-embedding-3-small\") -> np.ndarray:\n",
    "    if not isinstance(text, str):\n",
    "        raise ValueError(\"Input text must be a string.\")\n",
    "    if len(tokenizer.encode(text)) > MAX_TOKENS:\n",
    "        raise ValueError(\"Query too long!\")\n",
    "    response = client.embeddings.create(input=[text], model=model)\n",
    "    return np.array(response.data[0].embedding)\n",
    "df_laws = pd.read_excel('/content/drive/MyDrive/BM_Egypt_Law_Samples_500.xlsx')\n",
    "with open('/content/drive/MyDrive/EG_Legislations_BenchMark_1000.json', 'r', encoding='utf-8') as f:\n",
    "    questions_answers = json.load(f)\n",
    "\n",
    "law_texts = df_laws['Law Text'].tolist()\n",
    "law_text_embeddings = embed_law_texts(law_texts)\n",
    "bm25 = build_bm25_index(law_texts)\n",
    "\n",
    "ground_truth = {i: [i] for i in range(len(law_texts))}  \n",
    "\n",
    "bm25_ndcg10 = []\n",
    "bm25_ndcg100 = []\n",
    "bm25_map = []\n",
    "\n",
    "semantic_ndcg10 = []\n",
    "semantic_ndcg100 = []\n",
    "semantic_map = []\n",
    "\n",
    "hybrid_ndcg10 = []\n",
    "hybrid_ndcg100 = []\n",
    "hybrid_map = []\n",
    "\n",
    "for idx, qa in enumerate(questions_answers):\n",
    "    questions = qa['Questions']\n",
    "\n",
    "    for question in questions:\n",
    "        query_embedding = embed_query(question)\n",
    "\n",
    "        #BM25\n",
    "        bm25_top_indices, bm25_scores = bm25_search(question, bm25, k=100)\n",
    "\n",
    "        #Semantic\n",
    "        semantic_top_indices, semantic_scores = semantic_search(query_embedding, law_text_embeddings, top_k=100)\n",
    "\n",
    "        #Hybrid\n",
    "        hybrid_top_indices = rrf_fusion(bm25_top_indices, semantic_top_indices)\n",
    "\n",
    "        relevant_docs = ground_truth.get(idx, [])\n",
    "\n",
    "        \n",
    "        bm25_ndcg10.append(calculate_ndcg(relevant_docs, bm25_top_indices, 10))\n",
    "        bm25_ndcg100.append(calculate_ndcg(relevant_docs, bm25_top_indices, 100))\n",
    "        bm25_map.append(average_precision(relevant_docs, bm25_top_indices))\n",
    "\n",
    "        semantic_ndcg10.append(calculate_ndcg(relevant_docs, semantic_top_indices, 10))\n",
    "        semantic_ndcg100.append(calculate_ndcg(relevant_docs, semantic_top_indices, 100))\n",
    "        semantic_map.append(average_precision(relevant_docs, semantic_top_indices))\n",
    "\n",
    "        hybrid_ndcg10.append(calculate_ndcg(relevant_docs, hybrid_top_indices, 10))\n",
    "        hybrid_ndcg100.append(calculate_ndcg(relevant_docs, hybrid_top_indices, 100))\n",
    "        hybrid_map.append(average_precision(relevant_docs, hybrid_top_indices))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"=== Final Scores ===\")\n",
    "print(f\"BM25 -> nDCG@10: {np.mean(bm25_ndcg10):.4f}, nDCG@100: {np.mean(bm25_ndcg100):.4f}, MAP: {np.mean(bm25_map):.4f}\")\n",
    "print(f\"Semantic -> nDCG@10: {np.mean(semantic_ndcg10):.4f}, nDCG@100: {np.mean(semantic_ndcg100):.4f}, MAP: {np.mean(semantic_map):.4f}\")\n",
    "print(f\"Hybrid -> nDCG@10: {np.mean(hybrid_ndcg10):.4f}, nDCG@100: {np.mean(hybrid_ndcg100):.4f}, MAP: {np.mean(hybrid_map):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87479c61",
   "metadata": {
    "id": "F-Y1NNfXiXW6"
   },
   "source": [
    "### Arabic Morphology(Farasa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "639bf2f5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CTzjgumNaUtX",
    "outputId": "e0ffc07b-ba77-497d-b098-88254abc3d22"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-05-19 12:14:37,643 - farasapy_logger - WARNING]: Be careful with large lines as they may break on interactive mode. You may switch to Standalone mode for such cases.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Final Scores ===\n",
      "BM25 (Raw)       -> nDCG@10: 0.7551, nDCG@100: 0.7726, MAP: 0.7178\n",
      "BM25 (Processed) -> nDCG@10: 0.8041, nDCG@100: 0.8180, MAP: 0.7715\n",
      "Semantic         -> nDCG@10: 0.7924, nDCG@100: 0.8061, MAP: 0.7488\n",
      "Hybrid           -> nDCG@10: 0.8626, nDCG@100: 0.8702, MAP: 0.8304\n"
     ]
    }
   ],
   "source": [
    "farasa_segmenter = FarasaSegmenter(interactive=True)\n",
    "\n",
    "def preprocess_arabic(text: str) -> str:\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    return farasa_segmenter.segment(text)\n",
    "\n",
    "client = AzureOpenAI(\n",
    "    azure_endpoint=\"https://eslsca-openai.openai.azure.com/\",\n",
    "    api_key=\"0d368117945a4cb8a0f5b282dd192340\",\n",
    "    api_version=\"2024-05-01-preview\",\n",
    ")\n",
    "\n",
    "tokenizer = tiktoken.encoding_for_model(\"text-embedding-3-small\")\n",
    "MAX_TOKENS = 8191\n",
    "\n",
    "def embed_query(text: str, model=\"text-embedding-3-small\") -> np.ndarray:\n",
    "    if not isinstance(text, str):\n",
    "        raise ValueError(\"Input text must be a string.\")\n",
    "    if len(tokenizer.encode(text)) > MAX_TOKENS:\n",
    "        raise ValueError(\"Query too long!\")\n",
    "    response = client.embeddings.create(input=[text], model=model)\n",
    "    return np.array(response.data[0].embedding)\n",
    "\n",
    "\n",
    "df_laws = pd.read_excel('/content/drive/MyDrive/BM_Egypt_Law_Samples_500.xlsx')\n",
    "with open('/content/drive/MyDrive/EG_Legislations_BenchMark_1000.json', 'r', encoding='utf-8') as f:\n",
    "    questions_answers = json.load(f)\n",
    "\n",
    "law_texts = df_laws['Law Text'].tolist()\n",
    "\n",
    "law_text_embeddings = embed_law_texts(law_texts)\n",
    "\n",
    "bm25_raw = build_bm25_index(law_texts, preprocess=False)\n",
    "bm25_preprocessed = build_bm25_index(law_texts, preprocess=True)\n",
    "\n",
    "ground_truth = {i: [i] for i in range(len(law_texts))}\n",
    "\n",
    "raw_ndcg10, raw_ndcg100, raw_map = [], [], []\n",
    "proc_ndcg10, proc_ndcg100, proc_map = [], [], []\n",
    "semantic_ndcg10, semantic_ndcg100, semantic_map = [], [], []\n",
    "hybrid_ndcg10, hybrid_ndcg100, hybrid_map = [], [], []\n",
    "\n",
    "for idx, qa in enumerate(questions_answers):\n",
    "    questions = qa['Questions']\n",
    "    for question in questions:\n",
    "        query_embedding = embed_query(question)\n",
    "        relevant_docs = ground_truth.get(idx, [])\n",
    "\n",
    "        raw_indices, _ = bm25_search(question, bm25_raw, preprocess=False)\n",
    "        raw_ndcg10.append(calculate_ndcg(relevant_docs, raw_indices, 10))\n",
    "        raw_ndcg100.append(calculate_ndcg(relevant_docs, raw_indices, 100))\n",
    "        raw_map.append(average_precision(relevant_docs, raw_indices))\n",
    "\n",
    "        proc_indices, _ = bm25_search(question, bm25_preprocessed, preprocess=True)\n",
    "        proc_ndcg10.append(calculate_ndcg(relevant_docs, proc_indices, 10))\n",
    "        proc_ndcg100.append(calculate_ndcg(relevant_docs, proc_indices, 100))\n",
    "        proc_map.append(average_precision(relevant_docs, proc_indices))\n",
    "\n",
    "        semantic_indices, _ = semantic_search(query_embedding, law_text_embeddings)\n",
    "        semantic_ndcg10.append(calculate_ndcg(relevant_docs, semantic_indices, 10))\n",
    "        semantic_ndcg100.append(calculate_ndcg(relevant_docs, semantic_indices, 100))\n",
    "        semantic_map.append(average_precision(relevant_docs, semantic_indices))\n",
    "\n",
    "        hybrid_indices = rrf_fusion(proc_indices, semantic_indices)\n",
    "        hybrid_ndcg10.append(calculate_ndcg(relevant_docs, hybrid_indices, 10))\n",
    "        hybrid_ndcg100.append(calculate_ndcg(relevant_docs, hybrid_indices, 100))\n",
    "        hybrid_map.append(average_precision(relevant_docs, hybrid_indices))\n",
    "\n",
    "print(\"=== Final Scores ===\")\n",
    "print(f\"BM25 (Raw)       -> nDCG@10: {np.mean(raw_ndcg10):.4f}, nDCG@100: {np.mean(raw_ndcg100):.4f}, MAP: {np.mean(raw_map):.4f}\")\n",
    "print(f\"BM25 (Processed) -> nDCG@10: {np.mean(proc_ndcg10):.4f}, nDCG@100: {np.mean(proc_ndcg100):.4f}, MAP: {np.mean(proc_map):.4f}\")\n",
    "print(f\"Semantic         -> nDCG@10: {np.mean(semantic_ndcg10):.4f}, nDCG@100: {np.mean(semantic_ndcg100):.4f}, MAP: {np.mean(semantic_map):.4f}\")\n",
    "print(f\"Hybrid           -> nDCG@10: {np.mean(hybrid_ndcg10):.4f}, nDCG@100: {np.mean(hybrid_ndcg100):.4f}, MAP: {np.mean(hybrid_map):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f294f9e5",
   "metadata": {
    "id": "zQW2Q_PQ5IMI"
   },
   "source": [
    "### GTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e716f0c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a789fa09c2fb4c5292c8f0975a495841",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/611M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Uni\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Uni\\.cache\\huggingface\\hub\\models--Alibaba-NLP--gte-multilingual-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Some weights of the model checkpoint at Alibaba-NLP/gte-multilingual-base were not used when initializing NewModel: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing NewModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing NewModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Embedding laws: 100%|██████████| 500/500 [02:34<00:00,  3.23it/s]\n",
      "Evaluating queries: 100%|██████████| 1000/1000 [01:11<00:00, 14.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Final Scores ===\n",
      "BM25      → nDCG@10: 0.7564, nDCG@100: 0.7738, MAP: 0.7195\n",
      "Semantic  → nDCG@10: 0.6551, nDCG@100: 0.6818, MAP: 0.6092\n",
      "Hybrid    → nDCG@10: 0.7637, nDCG@100: 0.7865, MAP: 0.7273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"Alibaba-NLP/gte-multilingual-base\"\n",
    "tokenizer  = AutoTokenizer.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "model = AutoModel.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    trust_remote_code=True\n",
    ").eval().to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "MAX_TOKENS = tokenizer.model_max_length - 10 \n",
    "\n",
    "\n",
    "\n",
    "def embed_text(text: str) -> np.ndarray:\n",
    "    \"\"\"Embed a single text (question or law) via chunking + masked mean pooling.\"\"\"\n",
    "    pieces = (\n",
    "        split_text(text)\n",
    "        if len(tokenizer.encode(text, add_special_tokens=False)) > MAX_TOKENS\n",
    "        else [text]\n",
    "    )\n",
    "    embs = []\n",
    "    with torch.no_grad():\n",
    "        for piece in pieces:\n",
    "            inp = tokenizer(\n",
    "                piece,\n",
    "                truncation=True,\n",
    "                max_length=MAX_TOKENS,\n",
    "                return_tensors=\"pt\"\n",
    "            ).to(model.device)\n",
    "            outputs = model(**inp).last_hidden_state          \n",
    "            mask = inp.attention_mask.unsqueeze(-1).float()     \n",
    "            summed = (outputs * mask).sum(dim=1)               \n",
    "            counts = mask.sum(dim=1).clamp(min=1)               \n",
    "            pooled = (summed / counts).squeeze(0)            \n",
    "            embs.append(pooled.cpu().numpy())\n",
    "    return np.mean(np.stack(embs, axis=0), axis=0)\n",
    "\n",
    "excel_path = \"BM_Egypt_Law_Samples_500.xlsx\"\n",
    "df = pd.read_excel(excel_path)\n",
    "law_texts = df[\"Law Text\"].astype(str).tolist()\n",
    "\n",
    "law_embs = []\n",
    "for txt in tqdm(law_texts, desc=\"Embedding laws\"):\n",
    "    law_embs.append(embed_text(txt))\n",
    "law_embs = np.vstack(law_embs)\n",
    "\n",
    "tokenized_laws = [text.split() for text in law_texts]\n",
    "bm25 = BM25Okapi(tokenized_laws)\n",
    "\n",
    "json_path = \"EG_Legislations_BenchMark_1000.json\"\n",
    "with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    bm = json.load(f)\n",
    "\n",
    "queries = []\n",
    "for idx, obj in enumerate(bm):\n",
    "    for q in obj[\"Questions\"]:\n",
    "        queries.append((q, idx))\n",
    "\n",
    "results = {\n",
    "    \"bm25_ndcg10\": [], \"bm25_ndcg100\": [], \"bm25_map\": [],\n",
    "    \"sem_ndcg10\": [],  \"sem_ndcg100\": [],  \"sem_map\": [],\n",
    "    \"hybrid_ndcg10\": [], \"hybrid_ndcg100\": [], \"hybrid_map\": []\n",
    "}\n",
    "\n",
    "for query, gt_idx in tqdm(queries, desc=\"Evaluating queries\"):\n",
    "    q_emb = embed_text(query)\n",
    "    b_top = bm25_search(query, top_k=100)\n",
    "    s_top = semantic_search(q_emb, top_k=100)\n",
    "    h_top = rrf_fusion(b_top, s_top)\n",
    "\n",
    "    rel_docs = [gt_idx]\n",
    "\n",
    "    results[\"bm25_ndcg10\"].append(ndcg_at_k(rel_docs, b_top, 10))\n",
    "    results[\"bm25_ndcg100\"].append(ndcg_at_k(rel_docs, b_top, 100))\n",
    "    results[\"bm25_map\"].append(calculate_ap(rel_docs, b_top))\n",
    "    \n",
    "    results[\"sem_ndcg10\"].append(ndcg_at_k(rel_docs, s_top, 10))\n",
    "    results[\"sem_ndcg100\"].append(ndcg_at_k(rel_docs, s_top, 100))\n",
    "    results[\"sem_map\"].append(calculate_ap(rel_docs, s_top))\n",
    "    results[\"hybrid_ndcg10\"].append(ndcg_at_k(rel_docs, h_top, 10))\n",
    "    results[\"hybrid_ndcg100\"].append(ndcg_at_k(rel_docs, h_top, 100))\n",
    "    results[\"hybrid_map\"].append(calculate_ap(rel_docs, h_top))\n",
    "\n",
    "for k, vals in results.items():\n",
    "    results[k] = np.nan_to_num(vals)\n",
    "\n",
    "print(\"\\n=== Final Scores ===\")\n",
    "print(f\"BM25      → nDCG@10: {np.mean(results['bm25_ndcg10']):.4f}, \"\n",
    "      f\"nDCG@100: {np.mean(results['bm25_ndcg100']):.4f}, MAP: {np.mean(results['bm25_map']):.4f}\")\n",
    "print(f\"Semantic  → nDCG@10: {np.mean(results['sem_ndcg10']):.4f}, \"\n",
    "      f\"nDCG@100: {np.mean(results['sem_ndcg100']):.4f}, MAP: {np.mean(results['sem_map']):.4f}\")\n",
    "print(f\"Hybrid    → nDCG@10: {np.mean(results['hybrid_ndcg10']):.4f}, \"\n",
    "      f\"nDCG@100: {np.mean(results['hybrid_ndcg100']):.4f}, MAP: {np.mean(results['hybrid_map']):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8795f4",
   "metadata": {},
   "source": [
    "### BGE-M3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d542e1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding laws: 100%|██████████| 500/500 [05:54<00:00,  1.41it/s]\n",
      "Evaluating queries: 100%|██████████| 1000/1000 [02:59<00:00,  5.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Final Scores ===\n",
      "BM25      → nDCG@10: 0.7564, nDCG@100: 0.7738, MAP: 0.7195\n",
      "Semantic  → nDCG@10: 0.7872, nDCG@100: 0.8017, MAP: 0.7558\n",
      "Hybrid    → nDCG@10: 0.8126, nDCG@100: 0.8312, MAP: 0.7841\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"BAAI/bge-m3\"\n",
    "device     = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model     = AutoModel.from_pretrained(MODEL_NAME).to(device).eval()\n",
    "\n",
    "MAX_TOKENS = tokenizer.model_max_length - 10 \n",
    "excel_path = \"BM_Egypt_Law_Samples_500.xlsx\"\n",
    "df         = pd.read_excel(excel_path)\n",
    "law_texts  = df[\"Law Text\"].astype(str).tolist()\n",
    "\n",
    "law_embs = []\n",
    "for txt in tqdm(law_texts, desc=\"Embedding laws\"):\n",
    "    law_embs.append(embed_text(txt))\n",
    "law_embs = np.vstack(law_embs)\n",
    "\n",
    "tokenized_laws = [text.split() for text in law_texts]\n",
    "bm25 = BM25Okapi(tokenized_laws)\n",
    "\n",
    "json_path = \"EG_Legislations_BenchMark_1000.json\"\n",
    "with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    bm = json.load(f)\n",
    "\n",
    "queries = []\n",
    "for idx, obj in enumerate(bm):\n",
    "    for q in obj[\"Questions\"]:\n",
    "        queries.append((q, idx))\n",
    "\n",
    "results = {\n",
    "    \"bm25_ndcg10\": [], \"bm25_ndcg100\": [], \"bm25_map\": [],\n",
    "    \"sem_ndcg10\": [],  \"sem_ndcg100\": [],  \"sem_map\": [],\n",
    "    \"hybrid_ndcg10\": [], \"hybrid_ndcg100\": [], \"hybrid_map\": []\n",
    "}\n",
    "\n",
    "for query, gt_idx in tqdm(queries, desc=\"Evaluating queries\"):\n",
    "    q_emb = embed_text(query)\n",
    "    b_top = bm25_search(query, top_k=100)\n",
    "    s_top = semantic_search(q_emb, top_k=100)\n",
    "    h_top = rrf_fusion(b_top, s_top)\n",
    "\n",
    "    rel_docs = [gt_idx]\n",
    "\n",
    "    results[\"bm25_ndcg10\"].append(ndcg_at_k(rel_docs, b_top, 10))\n",
    "    results[\"bm25_ndcg100\"].append(ndcg_at_k(rel_docs, b_top, 100))\n",
    "    results[\"bm25_map\"].append(calculate_ap(rel_docs, b_top))\n",
    "    results[\"sem_ndcg10\"].append(ndcg_at_k(rel_docs, s_top, 10))\n",
    "    results[\"sem_ndcg100\"].append(ndcg_at_k(rel_docs, s_top, 100))\n",
    "    results[\"sem_map\"].append(calculate_ap(rel_docs, s_top))\n",
    "    results[\"hybrid_ndcg10\"].append(ndcg_at_k(rel_docs, h_top, 10))\n",
    "    results[\"hybrid_ndcg100\"].append(ndcg_at_k(rel_docs, h_top, 100))\n",
    "    results[\"hybrid_map\"].append(calculate_ap(rel_docs, h_top))\n",
    "\n",
    "for k, vals in results.items():\n",
    "    results[k] = np.nan_to_num(vals)\n",
    "print(\"\\n=== Final Scores ===\")\n",
    "print(f\"BM25      → nDCG@10: {np.mean(results['bm25_ndcg10']):.4f}, \"\n",
    "      f\"nDCG@100: {np.mean(results['bm25_ndcg100']):.4f}, MAP: {np.mean(results['bm25_map']):.4f}\")\n",
    "print(f\"Semantic  → nDCG@10: {np.mean(results['sem_ndcg10']):.4f}, \"\n",
    "      f\"nDCG@100: {np.mean(results['sem_ndcg100']):.4f}, MAP: {np.mean(results['sem_map']):.4f}\")\n",
    "print(f\"Hybrid    → nDCG@10: {np.mean(results['hybrid_ndcg10']):.4f}, \"\n",
    "      f\"nDCG@100: {np.mean(results['hybrid_ndcg100']):.4f}, MAP: {np.mean(results['hybrid_map']):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb605d6b",
   "metadata": {},
   "source": [
    "### Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db66ebd3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "KpkPOQeMWXU6",
    "outputId": "c34ea3cb-7287-4414-fe2d-b27ceb049d17"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   BM25_nDCG  Semantic_nDCG  Hybrid_nDCG  BM25_MAP  Semantic_MAP  Hybrid_MAP\n",
      "0   0.768336        0.02818     0.330348  0.718475      0.011345    0.189314\n"
     ]
    }
   ],
   "source": [
    "genai.configure(api_key=\"AIzaSyC-1eD9ThWOyHPGvMJ-ZZ1XYW371fjRkIU\")\n",
    "model = \"models/text-embedding-004\"\n",
    "\n",
    "def split_text(text, max_chars=8000):\n",
    "    chunks = []\n",
    "    while len(text) > max_chars:\n",
    "        split_point = text.rfind(\" \", 0, max_chars)\n",
    "        if split_point == -1:\n",
    "            split_point = max_chars\n",
    "        chunks.append(text[:split_point])\n",
    "        text = text[split_point:]\n",
    "    if text:\n",
    "        chunks.append(text)\n",
    "    return chunks\n",
    "\n",
    "def embed_query(text):\n",
    "    pieces = split_text(text)\n",
    "    embeddings = []\n",
    "    for chunk in pieces:\n",
    "        try:\n",
    "            response = genai.embed_content(\n",
    "                model=model,\n",
    "                content=chunk,\n",
    "                task_type=\"RETRIEVAL_DOCUMENT\"\n",
    "            )\n",
    "            embeddings.append(response['embedding'])\n",
    "            time.sleep(2)\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Embedding failed: {e}\")\n",
    "    return np.mean(np.stack(embeddings, axis=0), axis=0)\n",
    "\n",
    "\n",
    "df_laws = pd.read_excel('/content/drive/MyDrive/Colab Notebooks/BM_Egypt_Law_Samples_500.xlsx')\n",
    "with open('/content/drive/MyDrive/Colab Notebooks/EG_Legislations_BenchMark_1000.json', 'r', encoding='utf-8') as f:\n",
    "    questions_answers = json.load(f)\n",
    "\n",
    "law_texts = df_laws['Law Text'].tolist()\n",
    "law_text_embeddings = embed_law_texts(law_texts)\n",
    "bm25 = build_bm25_index(law_texts)\n",
    "\n",
    "ground_truth = {i: [i] for i in range(len(law_texts))} \n",
    "\n",
    "\n",
    "bm25_ndcg = {k: [] for k in range(10, 101, 10)}\n",
    "semantic_ndcg = {k: [] for k in range(10, 101, 10)}\n",
    "hybrid_ndcg = {k: [] for k in range(10, 101, 10)}\n",
    "\n",
    "bm25_map = []\n",
    "semantic_map = []\n",
    "hybrid_map = []\n",
    "\n",
    "for idx, qa in enumerate(questions_answers):\n",
    "    questions = qa['Questions']\n",
    "    for question in questions:\n",
    "        query_embedding = embed_query(question)\n",
    "\n",
    "        bm25_top_indices, bm25_scores = bm25_search(question, bm25, k=100)\n",
    "\n",
    "        semantic_top_indices, semantic_scores = semantic_search(query_embedding, law_text_embeddings, top_k=100)\n",
    "\n",
    "        hybrid_top_indices = rrf_fusion(bm25_top_indices, semantic_top_indices)\n",
    "\n",
    "        relevant_docs = ground_truth.get(idx, [])\n",
    "\n",
    "        for k in range(10, 101, 10):\n",
    "            bm25_ndcg[k].append(ndcg_at_k(relevant_docs, bm25_top_indices, k=k))\n",
    "            semantic_ndcg[k].append(ndcg_at_k(relevant_docs, semantic_top_indices, k=k))\n",
    "            hybrid_ndcg[k].append(ndcg_at_k(relevant_docs, hybrid_top_indices, k=k))\n",
    "\n",
    "        bm25_map.append(calculate_ap(relevant_docs, bm25_top_indices))\n",
    "        semantic_map.append(calculate_ap(relevant_docs, semantic_top_indices))\n",
    "        hybrid_map.append(calculate_ap(relevant_docs, hybrid_top_indices))\n",
    "\n",
    "def handle_nan_score(score):\n",
    "    return score if not np.isnan(score) else 0.0\n",
    "\n",
    "metrics = {\n",
    "    \"BM25_nDCG\": [np.nanmean([np.nanmean(bm25_ndcg[k]) for k in range(10, 101, 10)])],\n",
    "    \"Semantic_nDCG\": [np.nanmean([np.nanmean(semantic_ndcg[k]) for k in range(10, 101, 10)])],\n",
    "    \"Hybrid_nDCG\": [np.nanmean([np.nanmean(hybrid_ndcg[k]) for k in range(10, 101, 10)])],\n",
    "    \"BM25_MAP\": [np.nanmean(bm25_map)],\n",
    "    \"Semantic_MAP\": [np.nanmean(semantic_map)],\n",
    "    \"Hybrid_MAP\": [np.nanmean(hybrid_map)],\n",
    "}\n",
    "\n",
    "results_df = pd.DataFrame(metrics)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a42bcc",
   "metadata": {},
   "source": [
    "### Mistral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18caa471",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Gz7PoJGh78wJ",
    "outputId": "6abf3829-b6b9-440f-eb82-919048ea3466"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   BM25_nDCG  Semantic_nDCG  Hybrid_nDCG  BM25_MAP  Semantic_MAP  Hybrid_MAP\n",
      "0   0.768336       0.807883     0.848969  0.718475      0.757073    0.807296\n"
     ]
    }
   ],
   "source": [
    "api_key = \"S5y1cIeaYsoUSXCBkxM6fg6TC5FzRJVx\"\n",
    "model = \"mistral-embed\"\n",
    "client = Mistral(api_key=api_key)\n",
    "\n",
    "def embed_query(text):\n",
    "    pieces = split_text(text)\n",
    "    embeddings = []\n",
    "    for chunk in pieces:\n",
    "        try:\n",
    "            response = client.embeddings.create(\n",
    "                    model=model,\n",
    "                    inputs=chunk\n",
    "            )\n",
    "            embeddings.append(response.data[0].embedding)\n",
    "            time.sleep(2)\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Embedding failed: {e}\")\n",
    "    return np.mean(np.stack(embeddings, axis=0), axis=0)\n",
    "\n",
    "\n",
    "df_laws = pd.read_excel('/content/drive/MyDrive/BM_Egypt_Law_Samples_500.xlsx')\n",
    "with open('/content/drive/MyDrive/EG_Legislations_BenchMark_1000.json', 'r', encoding='utf-8') as f:\n",
    "    questions_answers = json.load(f)\n",
    "\n",
    "law_texts = df_laws['Law Text'].tolist()\n",
    "law_text_embeddings = embed_law_texts(law_texts)\n",
    "bm25 = build_bm25_index(law_texts)\n",
    "\n",
    "ground_truth = {i: [i] for i in range(len(law_texts))} \n",
    "\n",
    "bm25_ndcg = {k: [] for k in range(10, 101, 10)}\n",
    "semantic_ndcg = {k: [] for k in range(10, 101, 10)}\n",
    "hybrid_ndcg = {k: [] for k in range(10, 101, 10)}\n",
    "\n",
    "bm25_map = []\n",
    "semantic_map = []\n",
    "hybrid_map = []\n",
    "\n",
    "for idx, qa in enumerate(questions_answers):\n",
    "    questions = qa['Questions']\n",
    "    for question in questions:\n",
    "        query_embedding = embed_query(question)\n",
    "\n",
    "        bm25_top_indices, bm25_scores = bm25_search(question, bm25, k=100)\n",
    "\n",
    "        semantic_top_indices, semantic_scores = semantic_search(query_embedding, law_text_embeddings, top_k=100)\n",
    "\n",
    "        hybrid_top_indices = rrf_fusion(bm25_top_indices, semantic_top_indices)\n",
    "\n",
    "        relevant_docs = ground_truth.get(idx, [])\n",
    "\n",
    "        for k in range(10, 101, 10):\n",
    "            bm25_ndcg[k].append(ndcg_at_k(relevant_docs, bm25_top_indices, k=k))\n",
    "            semantic_ndcg[k].append(ndcg_at_k(relevant_docs, semantic_top_indices, k=k))\n",
    "            hybrid_ndcg[k].append(ndcg_at_k(relevant_docs, hybrid_top_indices, k=k))\n",
    "\n",
    "        bm25_map.append(calculate_ap(relevant_docs, bm25_top_indices))\n",
    "        semantic_map.append(calculate_ap(relevant_docs, semantic_top_indices))\n",
    "        hybrid_map.append(calculate_ap(relevant_docs, hybrid_top_indices))\n",
    "\n",
    "def handle_nan_score(score):\n",
    "    return score if not np.isnan(score) else 0.0\n",
    "\n",
    "metrics = {\n",
    "    \"BM25_nDCG\": [np.nanmean([np.nanmean(bm25_ndcg[k]) for k in range(10, 101, 10)])],\n",
    "    \"Semantic_nDCG\": [np.nanmean([np.nanmean(semantic_ndcg[k]) for k in range(10, 101, 10)])],\n",
    "    \"Hybrid_nDCG\": [np.nanmean([np.nanmean(hybrid_ndcg[k]) for k in range(10, 101, 10)])],\n",
    "    \"BM25_MAP\": [np.nanmean(bm25_map)],\n",
    "    \"Semantic_MAP\": [np.nanmean(semantic_map)],\n",
    "    \"Hybrid_MAP\": [np.nanmean(hybrid_map)],\n",
    "}\n",
    "\n",
    "results_df = pd.DataFrame(metrics)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85eb646e",
   "metadata": {
    "id": "47TTH9Php0i8"
   },
   "source": [
    "### E5-Large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aaa76421",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 403,
     "referenced_widgets": [
      "7b208d34e0664ef097244135a2845164",
      "ccbd0acc32e745ffa246029044d49edf",
      "073c01377452413a89bb7c1ba7f8ae29",
      "94cdc562d2134a57a2c57bb00aaee4b7",
      "0e717664123b4172b3a221ab04e33083",
      "53989f28d0cc447c9b087cad13f523c7",
      "9e955d2def86435f81386c3e35830558",
      "8f15b0538c184deea57f267f6815f10a",
      "fa0d8dbf39cf4bf69768e9786825a740",
      "83d8c865ed9940e58a576c136247d300",
      "16a716cd57274ceebf9a02f3ca53e7e8",
      "3b53b448430d4bbe972a6e7ecda548f6",
      "1462e13da4b641f5bcbb0b091fb52a41",
      "e880d85ed0cb485e8dee2ab3f97ea93d",
      "741030d2818d45848ba024761301d6c4",
      "1a4a59cb8d3c488897d6c0fae61920e6",
      "6c27597db56a497fb5cdf224fca84bfd",
      "87438b832e5c4e629d65dc7c490a38f5",
      "f277799692b24b56930fde8d3b1fba9b",
      "f9cb0f3967e04cb2be3f42d88eec0ab2",
      "2648cea1484b4403a291704d526e68e1",
      "95e7a9fd531c41fab226eecd88460643",
      "2efd35405696407fa57408e0acb20da5",
      "44f8e90e0dc3449697baf6a800576944",
      "638758a1fde545998abc7a675b7faa00",
      "74ffe23b5d0143ab9e10da7b0ff8a497",
      "46b19087b68e4d0abeda9f108d0fcab5",
      "f701903e23a14467a07553ec15885923",
      "f8c9d6c410704702b7a1b4eb9f22d406",
      "8a15e910031547cfb885b9180a3aa85f",
      "eeb386f69fc744ec8c424757a7535904",
      "dc2eaf844f7b4a5b9d13c7bb88a50eb8",
      "71aee6b106af44af8bab1e155a8b09a4",
      "377ad70fe66740cfac7ef383f021a7cf",
      "20ca8b6760fd41b193f1bac40e505d66",
      "8ad431326e8c48bab8cc8a25c47fd3b3",
      "67c9f90702ac499cb2f07f96defa4f51",
      "f30f13c851e241aca3b1490c64341693",
      "cc8b8d8d23ee4aecb1bc1cebc0153cff",
      "c79657471858420bb57d68a0995abb56",
      "8d39a8efd6d64dc6bb5e9b51f56c6989",
      "49b2d3d1ccf04c96a17b329112999105",
      "a07df82ab0914b35ac0097b7d2042d80",
      "4f973ba34c0e43d784ac705415b874d7",
      "ca19e1a6d20d46ce8fb5b3e06d49e4b8",
      "f65a8a6750af4b21a069dc9c10a7426e",
      "b5f0b9140618488f95e5d8f142c94974",
      "5e6d53f2ce33498e9b1e1718fd3966dd",
      "a5717124f90843de9a0064f93203b3f4",
      "ef8aebc7f7674f29aef1cbb292553dfe",
      "85b178319ce74977b417f6cf060797b5",
      "05ebe7678b874827820e6c7097e57892",
      "3c37fd520f9342bf9237939318147e0f",
      "073fbc7826f842b6b1cabb26b89b4c85",
      "86858b50169e48ac9c78145a7f11066b",
      "c8814e5a64344854b7f4346db0ba0985",
      "0dbd772949c84538adb56fd631ffeeba",
      "d66e75509b80433590f99ddb7b0b9f8b",
      "f66eff7e7c014bc9a74c196d9c13d3d1",
      "ebd2539d3d4b42e8b475e53629b45bf1",
      "7ce29b6603b04dfb9d88adcd362e68be",
      "710010dfb8fb43188cc138b14d75a536",
      "a1c1db82d9ca49b4865ee43067150e03",
      "25d253dfddee4e86a6e46dcdfc558c69",
      "7a0d2931c768406db1c78806edae2abe",
      "0ab13abf43104fae830b8676bf400843"
     ]
    },
    "id": "WN-aVKTapeHO",
    "outputId": "bc0b06d0-678b-4157-cce8-7f4365cc24de"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b208d34e0664ef097244135a2845164",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/418 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b53b448430d4bbe972a6e7ecda548f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2efd35405696407fa57408e0acb20da5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "377ad70fe66740cfac7ef383f021a7cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/280 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca19e1a6d20d46ce8fb5b3e06d49e4b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/690 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8814e5a64344854b7f4346db0ba0985",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.24G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Final Scores ===\n",
      "BM25 -> nDCG@10: 0.7551, nDCG@100: 0.7726, MAP: 0.7178\n",
      "Semantic -> nDCG@10: 0.9212, nDCG@100: 0.9235, MAP: 0.8986\n",
      "Hybrid -> nDCG@10: 0.8672, nDCG@100: 0.8751, MAP: 0.8378\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"intfloat/multilingual-e5-large\")\n",
    "model = AutoModel.from_pretrained(\"intfloat/multilingual-e5-large\")\n",
    "\n",
    "def embed_query(text: str) -> np.ndarray:\n",
    "    if not isinstance(text, str):\n",
    "        raise ValueError(\"Input text must be a string.\")\n",
    "    text = \"query: \" + text\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n",
    "    return embeddings[0]\n",
    "\n",
    "\n",
    "df_laws = pd.read_excel('/content/drive/MyDrive/BM_Egypt_Law_Samples_500.xlsx')\n",
    "with open('/content/drive/MyDrive/EG_Legislations_BenchMark_1000.json', 'r', encoding='utf-8') as f:\n",
    "    questions_answers = json.load(f)\n",
    "\n",
    "law_texts = df_laws['Law Text'].tolist()\n",
    "law_text_embeddings = embed_law_texts(law_texts)\n",
    "bm25 = build_bm25_index(law_texts)\n",
    "ground_truth = {i: [i] for i in range(len(law_texts))}\n",
    "\n",
    "bm25_ndcg10, bm25_ndcg100, bm25_map = [], [], []\n",
    "semantic_ndcg10, semantic_ndcg100, semantic_map = [], [], []\n",
    "hybrid_ndcg10, hybrid_ndcg100, hybrid_map = [], [], []\n",
    "\n",
    "for idx, qa in enumerate(questions_answers):\n",
    "    questions = qa['Questions']\n",
    "    for question in questions:\n",
    "        query_embedding = embed_query(question)\n",
    "        bm25_top_indices, _ = bm25_search(question, bm25, k=100)\n",
    "        semantic_top_indices, _ = semantic_search(query_embedding, law_text_embeddings, top_k=100)\n",
    "        hybrid_top_indices = rrf_fusion(bm25_top_indices, semantic_top_indices)\n",
    "        relevant_docs = ground_truth.get(idx, [])\n",
    "        total_docs = len(law_texts)\n",
    "        bm25_ndcg10.append(ndcg_at_k(relevant_docs, bm25_top_indices, 10, total_docs))\n",
    "        bm25_ndcg100.append(ndcg_at_k(relevant_docs, bm25_top_indices, 100, total_docs))\n",
    "        bm25_map.append(calculate_ap(relevant_docs, bm25_top_indices))\n",
    "        semantic_ndcg10.append(ndcg_at_k(relevant_docs, semantic_top_indices, 10, total_docs))\n",
    "        semantic_ndcg100.append(ndcg_at_k(relevant_docs, semantic_top_indices, 100, total_docs))\n",
    "        semantic_map.append(calculate_ap(relevant_docs, semantic_top_indices))\n",
    "        hybrid_ndcg10.append(ndcg_at_k(relevant_docs, hybrid_top_indices, 10, total_docs))\n",
    "        hybrid_ndcg100.append(ndcg_at_k(relevant_docs, hybrid_top_indices, 100, total_docs))\n",
    "        hybrid_map.append(calculate_ap(relevant_docs, hybrid_top_indices))\n",
    "\n",
    "print(\"=== Final Scores ===\")\n",
    "print(f\"BM25 -> nDCG@10: {np.mean(bm25_ndcg10):.4f}, nDCG@100: {np.mean(bm25_ndcg100):.4f}, MAP: {np.mean(bm25_map):.4f}\")\n",
    "print(f\"Semantic -> nDCG@10: {np.mean(semantic_ndcg10):.4f}, nDCG@100: {np.mean(semantic_ndcg100):.4f}, MAP: {np.mean(semantic_map):.4f}\")\n",
    "print(f\"Hybrid -> nDCG@10: {np.mean(hybrid_ndcg10):.4f}, nDCG@100: {np.mean(hybrid_ndcg100):.4f}, MAP: {np.mean(hybrid_map):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0330b2cf",
   "metadata": {},
   "source": [
    "###  Sentence Transformers with AraBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cd2bcd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 194,
     "referenced_widgets": [
      "6b3a96b58e004fcd92696296d67a0ba1",
      "8af0e1a45da74f8cb2c0b4764fe24832",
      "6ecf22b9497f4fbcac1f7937012b4701",
      "cfeca5e2d7824f4f8f5f1021dd21caac",
      "64c3739bd8ae466b8b1b892b753425e9",
      "eda23c455ffd4f4b8699b81e4305af8f",
      "68dbdbacb76846d2be64694421729c9b",
      "1424879557a14faa99505f435ad1cce9",
      "57d07b9824d042cc87e0ec320b5352b2",
      "497d83f83dc448dd8e86c7bf468dfe71",
      "44eb9aab64e4469da6cec5d286a54e8b",
      "11626621c884497990ddcd937bae6102",
      "555d8412d7484a708e7dff498a678ebb",
      "5c3bba7a99c34735a8bfa9b72a2ebcff",
      "73f7d84a889540ab86e09d6869a0fc3d",
      "9aaca72673154a9ab3ac8a595b77a1c3",
      "f2eb68ead1ec41da9a952d8f3854a60b",
      "5eb63aa179744db0a22b4a5071176d0b",
      "ed9002bce6b7449e8185c60ded108aa0",
      "383879e992e14b7c957e3165f41b28b9",
      "e2451cfbe36b451eaa2ca55021884949",
      "35aa680b8cc8451baf2e63dcf811c13f",
      "15880d8a3dd14d7ea5c6788f10806ff2",
      "3f186d07b09a4c53b6efbbf231143345",
      "a53ebd74650b4401af6a67a7fb839791",
      "bf73552176cb475495bcc3e7038e3b9f",
      "88bc9d6d13d549a2b5a04a0c140e5b7c",
      "00360792f7de4fd29990d53bca8c08e3",
      "eaf68d9834b24a0f950feb9c297ada5c",
      "e0201590a27f4147a03c717db5a4a821",
      "55fb40460b2748f18e186f1e240d6133",
      "cb62391c87b5410c8d5caee31fc3306c",
      "143e6bedca4a4f6d84a7d78e02f5c1f4",
      "f6fe03dd9c7a4bd5af047ede50fa1187",
      "bb41d6a331df4ad191c1cd98dae24eac",
      "edcfda78a91c46a3b479e136cd16c9b9",
      "1d75005fd0c9441e8cea00b984b71c19",
      "5f9e76af7944418ea95a784b69861a10",
      "7fc7fa857d134b91b007473964cfe00d",
      "976479b7f3f1497dbe73152eee2da636",
      "6f5593dcda76482cb0b150e055de7c64",
      "a85e7174bcbe49cb88fa73a29104a62f",
      "7da3aafab2d140c6aa5f177442c6c818",
      "75f30b0c9e6a44c3bb138d3ac2920f20",
      "01eb40e4e912447db1951bcaa491809f",
      "0fb9e7eccf0b4777af045ae3968b7b81",
      "cbc15e84b37a40e0b977d46be4ade4d5",
      "220bb1ea85a84fc4902de05109fbff71",
      "239416f2aeff45bcac04833c8484bd6d",
      "c3ed424498c942b59203124620ba628c",
      "ff2d6a21913b445e92bf1d6be59e56f1",
      "2dc5e10ed38f4c1393f8277b3cf5faad",
      "3427a77d55f740d19e0fba82721746d3",
      "054479f8f88b4dbb80d10cd90a9e77ac",
      "0a0c58547c06475f8093bafe26cd4f17"
     ]
    },
    "id": "uASS2Ls-5FIm",
    "outputId": "3059def6-c6af-4dc7-cc74-e43f5db9cd83"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b3a96b58e004fcd92696296d67a0ba1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/611 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11626621c884497990ddcd937bae6102",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/384 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15880d8a3dd14d7ea5c6788f10806ff2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/720k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6fe03dd9c7a4bd5af047ede50fa1187",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.31M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01eb40e4e912447db1951bcaa491809f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings saved to 'question_embeddings.npy' and 'law_text_embeddings.npy'\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"aubmindlab/bert-base-arabertv2\")\n",
    "model = SentenceTransformer('paraphrase-multilingual-mpnet-base-v2') \n",
    "\n",
    "def encode_with_arabert(texts):\n",
    "    encoded_input = tokenizer(texts, padding=True, truncation=True, max_length=512, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        model_output = model.encode(texts)\n",
    "    return model_output\n",
    "\n",
    "questions = [ex['question'] for ex in train_data_json]\n",
    "law_texts = [ex['law_text'] for ex in train_data_json]\n",
    "\n",
    "question_embeddings = encode_with_arabert(questions)\n",
    "law_text_embeddings = encode_with_arabert(law_texts)\n",
    "\n",
    "np.save(\"/content/drive/MyDrive/question_embeddings.npy\", question_embeddings)\n",
    "np.save(\"/content/drive/MyDrive/law_text_embeddings.npy\", law_text_embeddings)\n",
    "print(\"Embeddings saved to 'question_embeddings.npy' and 'law_text_embeddings.npy'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0b9335",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_h46L5PBiWYD",
    "outputId": "07a74b50-951d-456d-e11b-63936b31e7ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Semantic Search Results For Sentence Transformer ===\n",
      "nDCG@10:  0.2613\n",
      "nDCG@100: 0.2988\n",
      "MAP:      0.2258\n"
     ]
    }
   ],
   "source": [
    "question_embeddings = np.load(\"/content/drive/MyDrive/question_embeddings.npy\")[:500]\n",
    "law_text_embeddings = np.load(\"/content/drive/MyDrive/law_text_embeddings.npy\")[:500]\n",
    "df_laws = pd.read_excel('/content/drive/MyDrive/BM_Egypt_Law_Samples_500.xlsx')\n",
    "law_texts = df_laws['Law Text'].tolist()[:500] \n",
    "with open('/content/drive/MyDrive/EG_Legislations_BenchMark_1000.json', 'r', encoding='utf-8') as f:\n",
    "    questions_answers = json.load(f)\n",
    "questions_answers = questions_answers[:500]\n",
    "\n",
    "ground_truth = {i: [i] for i in range(500)}\n",
    "\n",
    "semantic_ndcg10 = []\n",
    "semantic_ndcg100 = []\n",
    "semantic_map = []\n",
    "\n",
    "for i, q_embed in enumerate(question_embeddings):\n",
    "    relevant_docs = ground_truth.get(i, [])\n",
    "\n",
    "    similarities = cosine_similarity([q_embed], law_text_embeddings)[0]\n",
    "    ranked_indices = np.argsort(similarities)[::-1][:100]\n",
    "    semantic_ndcg10.append(ndcg_at_k(relevant_docs, ranked_indices, k=10))\n",
    "    semantic_ndcg100.append(ndcg_at_k(relevant_docs, ranked_indices, k=100))\n",
    "    semantic_map.append(calculate_ap(relevant_docs, ranked_indices))\n",
    "\n",
    "print(\"\\n=== Semantic Search Results For Sentence Transformer ===\")\n",
    "print(f\"nDCG@10:  {np.nanmean(semantic_ndcg10):.4f}\")\n",
    "print(f\"nDCG@100: {np.nanmean(semantic_ndcg100):.4f}\")\n",
    "print(f\"MAP:      {np.nanmean(semantic_map):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe48daa",
   "metadata": {
    "id": "_6iZXBUMd4WS"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6a17d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47689da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
