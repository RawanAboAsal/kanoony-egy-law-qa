{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97354558",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import faiss\n",
    "import gradio as gr\n",
    "from openai import AzureOpenAI\n",
    "import tiktoken\n",
    "import json\n",
    "import csv\n",
    "import time\n",
    "import google.generativeai as genai\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "from datasets import Dataset\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from torch.utils.data import Dataset\n",
    "from rank_bm25 import BM25Okapi\n",
    "from transformers import AutoTokenizer, T5ForConditionalGeneration\n",
    "from rouge_score import rouge_scorer\n",
    "from bert_score import BERTScorer\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from transformers import AutoModelForSeq2SeqLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235286c6",
   "metadata": {},
   "source": [
    "### Using OpenAI API\n",
    "### RAG with FAISS based L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2c8cf4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 646
    },
    "id": "cNnR-WLVHO7Q",
    "outputId": "fa9ef9c2-a3e9-4ce4-fc14-89c8b3668151"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
      "\n",
      "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
      "* Running on public URL: https://cb7025e4942d40364b.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://cb7025e4942d40364b.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load articles and embeddings\n",
    "with open(\"/content/drive/MyDrive/articles.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    articles = json.load(f)\n",
    "\n",
    "embeddings = np.load(\"/content/drive/MyDrive/law_embeddings.npy\")\n",
    "dimension = embeddings.shape[1]\n",
    "\n",
    "# FAISS index setup\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(embeddings)\n",
    "\n",
    "# Azure OpenAI setup\n",
    "client = AzureOpenAI(\n",
    "    azure_endpoint=\"https://eslsca-openai.openai.azure.com/\",\n",
    "    api_key=\"\",\n",
    "    api_version=\"2024-05-01-preview\",\n",
    ")\n",
    "\n",
    "tokenizer = tiktoken.encoding_for_model(\"text-embedding-3-small\")\n",
    "MAX_TOKENS = 8191\n",
    "\n",
    "def embed_query(text, model=\"text-embedding-3-small\"):\n",
    "    if len(tokenizer.encode(text)) > MAX_TOKENS:\n",
    "        raise ValueError(\"Query too long!\")\n",
    "    response = client.embeddings.create(input=[text], model=model)\n",
    "    return np.array(response.data[0].embedding)\n",
    "\n",
    "def search(query, top_k=5):\n",
    "    query_emb = embed_query(query).reshape(1, -1)\n",
    "    distances, indices = index.search(query_emb, top_k)\n",
    "    return [articles[i][\"Law_Text\"] for i in indices[0]]\n",
    "\n",
    "def call_gpt_azure(SYS_PROMPT, USER_PROMPT):\n",
    "    message_text = [\n",
    "        {\"role\": \"system\", \"content\": SYS_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": USER_PROMPT},\n",
    "    ]\n",
    "\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=message_text,\n",
    "        temperature=0.0,\n",
    "        top_p=0.95,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0,\n",
    "        stop=None,\n",
    "    )\n",
    "    return completion.choices[0].message.content\n",
    "\n",
    "SYS_PROMPT = \"\"\"\n",
    "You are a highly skilled legal assistant specialized in Egyptian law.\n",
    "\n",
    "Your tasks:\n",
    "- If the user says anything that is not a legal question (greeting, small talk, etc.), reply in Arabic:\n",
    "مرحبًا! كيف أستطيع مساعدتك اليوم في استفسار قانوني؟\n",
    "\n",
    "- If the user asks a legal question:\n",
    "    - Automatically determine the correct relevant law name, article number, and law type (criminal, civil, administrative, etc.).\n",
    "    - Clearly identify the article number, law name, and law type.\n",
    "    - Provide a detailed and legally sound answer based strictly on the article's content and Egyptian legal principles.\n",
    "    - Inside the explanation, if it improves clarity, you may use bullet points (•) in Arabic to organize information.\n",
    "    - Bullet points must only appear inside the detailed answer, never outside or before starting the response.\n",
    "\n",
    "Format your response in Arabic, starting exactly like this:\n",
    "طبقًا للمادة [رقم المادة] من قانون [اسم القانون]، [نوع القانون]، [الإجابة التفصيلية].\n",
    "\n",
    "Rules:\n",
    "- Begin always with the sentence: طبقًا للمادة [رقم المادة] من قانون [اسم القانون]، [نوع القانون]، then continue.\n",
    "- Use plain Arabic text only — no English, no asterisks (*), no markdown, no code block formatting.\n",
    "- Bullet points inside the answer must use (•) and be clear and well-organized.\n",
    "- The legal explanation must be sufficiently detailed and professional.\n",
    "- Ensure the law, article, and law type are accurate.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "# Gradio Interface\n",
    "def legal_advice(query):\n",
    "    return generate_answer(query)\n",
    "css=\"\"\"\n",
    "@import url('https://fonts.googleapis.com/css2?family=Cairo&display=swap');\n",
    "\n",
    "html, body {\n",
    "  direction: rtl;\n",
    "  text-align: right;\n",
    "}\n",
    "\n",
    ".gradio-container {\n",
    "  font-family: 'Cairo', sans-serif;\n",
    "}\n",
    "\n",
    ".gradio-container textarea,\n",
    ".gradio-container input[type=\"text\"] {\n",
    "  direction: rtl;\n",
    "  text-align: right;\n",
    "  font-family: 'Cairo', sans-serif;\n",
    "}\n",
    "\n",
    "#answer-output {\n",
    "  direction: rtl;\n",
    "  text-align: right;\n",
    "  font-family: 'Cairo', sans-serif;\n",
    "  padding-right: 20px;\n",
    "  line-height: 1.8;\n",
    "}\n",
    "\n",
    "#answer-output ul {\n",
    "  list-style-type: none;\n",
    "  padding-left: 0;\n",
    "}\n",
    "#answer-output li {\n",
    "  margin-bottom: 15px;\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# Update the function to format the answer for Arabic\n",
    "def generate_answer(query):\n",
    "    # Step 1: Search relevant articles\n",
    "    contexts = search(query, top_k=3)\n",
    "\n",
    "    # Step 2: Merge contexts together\n",
    "    merged_context = \"\\n\\n\".join(contexts)\n",
    "\n",
    "    # Step 3: Construct User Prompt\n",
    "    USER_PROMPT = f\"المحتوى:\\n{merged_context}\\n\\nالسؤال:\\n{query}\"\n",
    "\n",
    "    # Step 4: Call GPT to generate the legal answer\n",
    "    answer = call_gpt_azure(SYS_PROMPT, USER_PROMPT)\n",
    "\n",
    "    # Remove any list bullet points or asterisks from the answer and replace with Arabic numerals\n",
    "    answer = answer.replace(\"*\", \"\")  # Removing the asterisks\n",
    "    answer = answer.replace(\"1.\", \"١.\").replace(\"2.\", \"٢.\").replace(\"3.\", \"٣.\").replace(\"4.\", \"٤.\")\n",
    "\n",
    "    # Ensuring proper right-aligned and readable format\n",
    "    answer = f\" {answer}\"\n",
    "\n",
    "    return answer\n",
    "\n",
    "# Gradio Interface\n",
    "iface = gr.Interface(\n",
    "    fn=legal_advice,\n",
    "    inputs=gr.Textbox(label=\"أدخل سؤالك القانوني\", elem_id=\"query-input\", lines=4, placeholder=\"اكتب سؤالك هنا...\"),\n",
    "    outputs=gr.Textbox(label=\"الإجابة القانونية\", elem_id=\"answer-output\", lines=6, placeholder=\"الجواب سيظهر هنا...\"),\n",
    "    title=\"نظام استشارات قانونية - القانون المصري\",\n",
    "    description=\"أدخل سؤالك القانوني المتعلق بالقانون المصري وسوف تتلقى إجابة بناءً على المواد القانونية ذات الصلة.\",\n",
    "    css=css\n",
    ")\n",
    "\n",
    "# Launch the Gradio interface\n",
    "iface.launch()\n",
    "\n",
    "# ما هي أنواع الأسلحة التي تُعتبر غير تقليدية وما تأثيرها المحتمل على الأمن العام؟\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b335320",
   "metadata": {},
   "source": [
    "### Using OpenAI API\n",
    "### RAG with FAISS based IP (Cosine similarity)_KnowledgeBase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5814072f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 646
    },
    "id": "cNnR-WLVHO7Q",
    "outputId": "fa9ef9c2-a3e9-4ce4-fc14-89c8b3668151"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
      "\n",
      "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
      "* Running on public URL: https://cb7025e4942d40364b.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://cb7025e4942d40364b.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"/content/drive/MyDrive/articles.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    articles = json.load(f)\n",
    "\n",
    "embeddings = np.load(\"/content/drive/MyDrive/law_embeddings.npy\")\n",
    "dimension = embeddings.shape[1]\n",
    "\n",
    "# Normalize embeddings for cosine similarity\n",
    "embeddings = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "\n",
    "# Cosine similarity FAISS index (inner product with normalized vectors)\n",
    "index = faiss.IndexFlatIP(dimension)\n",
    "index.add(embeddings)\n",
    "\n",
    "# Azure OpenAI setup\n",
    "client = AzureOpenAI(\n",
    "    azure_endpoint=\"https://eslsca-openai.openai.azure.com/\",\n",
    "    api_key=\"\",\n",
    "    api_version=\"2024-05-01-preview\",\n",
    ")\n",
    "\n",
    "# Tokenizer for input length check\n",
    "tokenizer = tiktoken.encoding_for_model(\"text-embedding-3-small\")\n",
    "MAX_TOKENS = 8191\n",
    "\n",
    "def embed_query(text, model=\"text-embedding-3-small\"):\n",
    "    if len(tokenizer.encode(text)) > MAX_TOKENS:\n",
    "        raise ValueError(\"Query too long!\")\n",
    "    response = client.embeddings.create(input=[text], model=model)\n",
    "    emb = np.array(response.data[0].embedding)\n",
    "    emb = emb / np.linalg.norm(emb)  # Normalize for cosine similarity\n",
    "    return emb\n",
    "\n",
    "def search(query, top_k=5):\n",
    "    query_emb = embed_query(query).reshape(1, -1)\n",
    "    scores, indices = index.search(query_emb, top_k)\n",
    "    return [articles[i][\"Law_Text\"] for i in indices[0]]\n",
    "\n",
    "def call_gpt_azure(SYS_PROMPT, USER_PROMPT):\n",
    "    message_text = [\n",
    "        {\"role\": \"system\", \"content\": SYS_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": USER_PROMPT},\n",
    "    ]\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=message_text,\n",
    "        temperature=0.0,\n",
    "        top_p=0.95,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0,\n",
    "    )\n",
    "    return completion.choices[0].message.content\n",
    "\n",
    "SYS_PROMPT = \"\"\"\n",
    "You are a highly skilled legal assistant specialized in Egyptian law.\n",
    "\n",
    "Your tasks:\n",
    "- If the user says anything that is not a legal question (greeting, small talk, etc.), reply in Arabic:\n",
    "مرحبًا! كيف أستطيع مساعدتك اليوم في استفسار قانوني؟\n",
    "\n",
    "- If the user asks a legal question:\n",
    "    - Automatically determine the correct relevant law name, article number, and law type (criminal, civil, administrative, etc.).\n",
    "    - Clearly identify the article number, law name, and law type.\n",
    "    - Provide a detailed and legally sound answer based strictly on the article's content and Egyptian legal principles.\n",
    "    - Inside the explanation, if it improves clarity, you may use bullet points (•) in Arabic to organize information.\n",
    "    - Bullet points must only appear inside the detailed answer, never outside or before starting the response.\n",
    "\n",
    "Format your response in Arabic, starting exactly like this:\n",
    "طبقًا للمادة [رقم المادة] من قانون [اسم القانون]، [نوع القانون]، [الإجابة التفصيلية].\n",
    "\n",
    "Rules:\n",
    "- Begin always with the sentence: طبقًا للمادة [رقم المادة] من قانون [اسم القانون]، [نوع القانون]، then continue.\n",
    "- Use plain Arabic text only — no English, no asterisks (*), no markdown, no code block formatting.\n",
    "- Bullet points inside the answer must use (•) and be clear and well-organized.\n",
    "- The legal explanation must be sufficiently detailed and professional.\n",
    "- Ensure the law, article, and law type are accurate.\n",
    "\"\"\"\n",
    "\n",
    "def generate_answer(query):\n",
    "    # Step 1: Search relevant articles\n",
    "    contexts = search(query, top_k=3)\n",
    "\n",
    "    # Step 2: Merge contexts\n",
    "    merged_context = \"\\n\\n\".join(contexts)\n",
    "\n",
    "    # Step 3: Build prompt\n",
    "    USER_PROMPT = f\"المحتوى:\\n{merged_context}\\n\\nالسؤال:\\n{query}\"\n",
    "\n",
    "    # Step 4: Get GPT answer\n",
    "    answer = call_gpt_azure(SYS_PROMPT, USER_PROMPT)\n",
    "\n",
    "    # Step 5: Replace bullets and numbers\n",
    "    answer = answer.replace(\"*\", \"\")\n",
    "    answer = answer.replace(\"1.\", \"١.\").replace(\"2.\", \"٢.\").replace(\"3.\", \"٣.\").replace(\"4.\", \"٤.\")\n",
    "    return f\" {answer}\"\n",
    "\n",
    "# Gradio Interface\n",
    "css = \"\"\"\n",
    "@import url('https://fonts.googleapis.com/css2?family=Cairo&display=swap');\n",
    "html, body { direction: rtl; text-align: right; }\n",
    ".gradio-container { font-family: 'Cairo', sans-serif; }\n",
    ".gradio-container textarea, .gradio-container input[type=\"text\"] {\n",
    "  direction: rtl; text-align: right; font-family: 'Cairo', sans-serif;\n",
    "}\n",
    "#answer-output {\n",
    "  direction: rtl; text-align: right; font-family: 'Cairo', sans-serif;\n",
    "  padding-right: 20px; line-height: 1.8;\n",
    "}\n",
    "#answer-output ul { list-style-type: none; padding-left: 0; }\n",
    "#answer-output li { margin-bottom: 15px; }\n",
    "\"\"\"\n",
    "\n",
    "iface = gr.Interface(\n",
    "    fn=generate_answer,\n",
    "    inputs=gr.Textbox(label=\"أدخل سؤالك القانوني\", elem_id=\"query-input\", lines=4, placeholder=\"اكتب سؤالك هنا...\"),\n",
    "    outputs=gr.Textbox(label=\"الإجابة القانونية\", elem_id=\"answer-output\", lines=6, placeholder=\"الجواب سيظهر هنا...\"),\n",
    "    title=\"نظام استشارات قانونية - القانون المصري\",\n",
    "    description=\"أدخل سؤالك القانوني المتعلق بالقانون المصري وسوف تتلقى إجابة بناءً على المواد القانونية ذات الصلة.\",\n",
    "    css=css\n",
    ")\n",
    "\n",
    "iface.launch()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f796d6",
   "metadata": {},
   "source": [
    "### Using Gemini API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc1c902",
   "metadata": {
    "id": "Rg38RxM0RC1J"
   },
   "source": [
    "#### With Gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d382c73",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 715
    },
    "id": "42YlqNC_gy4U",
    "outputId": "95494580-ca45-4337-dd63-c29d2e79b3ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
      "\n",
      "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
      "* Running on public URL: https://1d3aeb57787e614c39.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://1d3aeb57787e614c39.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "طبقًا للمادة ١٥ من قانون السلطة القضائية، قانون مدني، فإن المكتب الفني لمحكمة النقض يتم إلحاقه بالمحكمة ويتكون من عدد كاف من رؤساء المحكمة وقضاتها. وتتحدد مهام هذا المكتب الأساسية في الآتي:\n",
      "• إعداد التقارير في الطعون المعروضة على المحكمة، ويشمل ذلك فحص الأوراق وتلخيص الوقائع ووجوه الطعن والرد عليها وعرض السوابق القضائية ذات الصلة.\n",
      "• استخلاص المبادئ القانونية التي تقررها أحكام المحكمة وتبويبها، أي تحليل الأحكام النهائية للمحكمة لاستخلاص القواعد والمبادئ القانونية المستقرة وتصنيفها موضوعياً.\n",
      "• إعداد هذه المبادئ للنشر في مجموعات الأحكام التي تصدرها المحكمة، لضمان نشر السوابق القضائية وتيسير الرجوع إليها من قبل القضاة والمحامين والباحثين القانونيين.\n"
     ]
    }
   ],
   "source": [
    "# === Load data and setup FAISS index ===\n",
    "with open(\"/content/drive/MyDrive/filtered_articles.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    articles = json.load(f)\n",
    "\n",
    "embeddings = np.load(\"/content/drive/MyDrive/gemini_law_embeddings.npy\")\n",
    "dimension = embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(embeddings)\n",
    "\n",
    "# === Configure Gemini ===\n",
    "genai.configure(api_key=\"AIzaSyBPlT5IRCJ5UPyAQC_-4NAVri2zAPlRxas\")\n",
    "gemini_model = genai.GenerativeModel(\"models/gemini-2.5-flash-preview-04-17\")\n",
    "MODEL_NAME = \"models/text-embedding-004\"\n",
    "\n",
    "# === Prompt Setup ===\n",
    "SYS_PROMPT = \"\"\"\n",
    "You are a highly skilled legal assistant specialized in Egyptian law.\n",
    "\n",
    "Your tasks:\n",
    "- If the user says anything that is not a legal question (greeting, small talk, etc.), reply in Arabic:\n",
    "مرحبًا! كيف أستطيع مساعدتك اليوم في استفسار قانوني؟\n",
    "\n",
    "- If the user asks a legal question:\n",
    "    - Automatically determine the correct relevant law name, article number, and law type (criminal, civil, administrative, etc.).\n",
    "    - Clearly identify the article number, law name, and law type.\n",
    "    - Provide a detailed and legally sound answer based strictly on the article's content and Egyptian legal principles.\n",
    "    - Inside the explanation, if it improves clarity, you may use bullet points (•) in Arabic to organize information.\n",
    "    - Bullet points must only appear inside the detailed answer, never outside or before starting the response.\n",
    "\n",
    "Format your response in Arabic, starting exactly like this:\n",
    "طبقًا للمادة [رقم المادة] من قانون [اسم القانون]، [نوع القانون]، [الإجابة التفصيلية].\n",
    "\n",
    "Rules:\n",
    "- Begin always with the sentence: طبقًا للمادة [رقم المادة] من قانون [اسم القانون]، [نوع القانون]، then continue.\n",
    "- Use plain Arabic text only — no English, no asterisks (*), no markdown, no code block formatting.\n",
    "- Bullet points inside the answer must use (•) and be clear and well-organized.\n",
    "- The legal explanation must be sufficiently detailed and professional.\n",
    "- Ensure the law, article, and law type are accurate.\n",
    "\"\"\"\n",
    "\n",
    "# === Embedding and Search ===\n",
    "def embed_query(text: str) -> np.ndarray:\n",
    "    try:\n",
    "        response = genai.embed_content(\n",
    "            model=MODEL_NAME,\n",
    "            content=text,\n",
    "            task_type=\"retrieval_query\"\n",
    "        )\n",
    "\n",
    "        return np.array(response[\"embedding\"], dtype=np.float32)\n",
    "    except Exception as e:\n",
    "        print(f\"Embedding error: {e}\")\n",
    "        return np.zeros((dimension,), dtype=np.float32)\n",
    "\n",
    "def search(query: str, top_k=5):\n",
    "    try:\n",
    "        query_emb = embed_query(query).reshape(1, -1)\n",
    "        distances, indices = index.search(query_emb, top_k)\n",
    "        return [articles[i][\"Law_Text\"] for i in indices[0]]\n",
    "    except Exception as e:\n",
    "        print(f\"Search error: {e}\")\n",
    "        return []\n",
    "\n",
    "# === Generation ===\n",
    "def call_gemini(sys_prompt, user_prompt):\n",
    "    try:\n",
    "        response = gemini_model.generate_content([sys_prompt, user_prompt])\n",
    "        return response.text\n",
    "    except Exception as e:\n",
    "        print(f\"Gemini error: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# === Final Legal Advice Function ===\n",
    "def legal_advice(query):\n",
    "    contexts = search(query, top_k=5)\n",
    "    if not contexts:\n",
    "        return \"لم يتم العثور على مواد قانونية مرتبطة.\"\n",
    "\n",
    "    merged_context = \"\\n\\n\".join(contexts)\n",
    "    USER_PROMPT = f\"المحتوى:\\n{merged_context}\\n\\nالسؤال:\\n{query}\"\n",
    "    answer = call_gemini(SYS_PROMPT, USER_PROMPT)\n",
    "\n",
    "    # Format Arabic numerals\n",
    "    for i in range(1, 10):\n",
    "        answer = answer.replace(f\"{i}.\", f\"{chr(1632 + i)}.\")\n",
    "\n",
    "    return answer\n",
    "\n",
    "# === RTL UI with Gradio ===\n",
    "css = \"\"\"\n",
    "@import url('https://fonts.googleapis.com/css2?family=Cairo&display=swap');\n",
    "\n",
    "html, body {\n",
    "  direction: rtl;\n",
    "  text-align: right;\n",
    "}\n",
    "\n",
    ".gradio-container {\n",
    "  font-family: 'Cairo', sans-serif;\n",
    "}\n",
    "\n",
    ".gradio-container textarea,\n",
    ".gradio-container input[type=\"text\"] {\n",
    "  direction: rtl;\n",
    "  text-align: right;\n",
    "  font-family: 'Cairo', sans-serif;\n",
    "}\n",
    "\n",
    "#answer-output {\n",
    "  direction: rtl;\n",
    "  text-align: right;\n",
    "  font-family: 'Cairo', sans-serif;\n",
    "  padding-right: 20px;\n",
    "  line-height: 1.8;\n",
    "}\n",
    "\n",
    "#answer-output ul {\n",
    "  list-style-type: none;\n",
    "  padding-left: 0;\n",
    "}\n",
    "#answer-output li {\n",
    "  margin-bottom: 15px;\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# === Gradio Interface ===\n",
    "iface = gr.Interface(\n",
    "    fn=legal_advice,\n",
    "    inputs=gr.Textbox(label=\"أدخل سؤالك القانوني\", elem_id=\"query-input\", lines=4, placeholder=\"اكتب سؤالك هنا...\"),\n",
    "    outputs=gr.Textbox(label=\"الإجابة القانونية\", elem_id=\"answer-output\", lines=8, placeholder=\"الجواب سيظهر هنا...\"),\n",
    "    title=\"نظام استشارات قانونية - القانون المصري\",\n",
    "    description=\"أدخل سؤالك القانوني المتعلق بالقانون المصري وسوف تتلقى إجابة بناءً على المواد القانونية ذات الصلة.\",\n",
    "    css=css\n",
    ")\n",
    "\n",
    "iface.launch()\n",
    "# print(legal_advice(\"ما هي المهام الأساسية التي يقوم بها المكتب الفني للمبادئ القانونية في محكمة النقض؟\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54df7429",
   "metadata": {},
   "source": [
    "#### Without Gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69342d24",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 263
    },
    "id": "xg9NxUwdwL59",
    "outputId": "b121b541-27ce-44b2-8d4a-5fae6e384460"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "طبقًا للمادة ٣٠ من قانون نظام السلك الدبلوماسي والقنصلى، قانون إداري، الفقرة المضافة إلى هذه المادة تضع شرطًا إضافيًا للترقية إلى وظيفة مستشار في السلك الدبلوماسي والقنصلي.\n",
      "\n",
      "وينص التعديل المضاف إلى المادة على أنه:\n",
      "• في جميع الحالات التي تتم فيها الترقية إلى وظيفة مستشار، سواء كانت هذه الترقية بناءً على قاعدة الأقدمية (أي بسبب المدة الزمنية التي قضاها الموظف في الدرجة السابقة) أو بناءً على قاعدة الاختيار (أي بناءً على تقييم الكفاءة والملاءمة للوظيفة الأعلى)، فإنه لا يجوز إتمام هذه الترقية.\n",
      "• يشترط لإتمام الترقية اجتياز دورة تدريبية معينة.\n",
      "• هذه الدورة التدريبية يتم تنظيمها وعقدها خصيصًا لهذا الغرض بواسطة الوزارة المختصة (وهي وزارة الخارجية في هذا السياق).\n",
      "• أحال القانون في تفاصيل هذه الدورة التدريبية إلى اللائحة التنفيذية للقانون.\n",
      "• تحدد اللائحة التنفيذية لهذا القانون عدة جوانب تتعلق بالدورة التدريبية، وهي:\n",
      "    • مدة الدورة الزمنية.\n",
      "    • شروط محددة وأوضاع معينة يجب الالتزام بها لاجتياز الدورة بنجاح.\n",
      "    • الآثار والعواقب الأخرى التي قد تترتب على عدم اجتياز هذه الدورة التدريبية بنجاح.\n",
      "\n",
      "بناءً على ذلك، فإن الترقية إلى درجة مستشار في السلك الدبلوماسي والقنصلي تتوقف وجوبًا على النجاح في الدورة التدريبية التي تنظمها وزارة الخارجية، وتفاصيل هذه الدورة وشروط اجتيازها وآثار عدم اجتيازها تحددها اللائحة التنفيذية للقانون.\n"
     ]
    }
   ],
   "source": [
    "# === Load data and setup FAISS index ===\n",
    "with open(\"/content/drive/MyDrive/filtered_articles.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    articles = json.load(f)\n",
    "\n",
    "embeddings = np.load(\"/content/drive/MyDrive/gemini_law_embeddings.npy\")\n",
    "dimension = embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(embeddings)\n",
    "\n",
    "# === Configure Gemini ===\n",
    "genai.configure(api_key=\"AIzaSyBPlT5IRCJ5UPyAQC_-4NAVri2zAPlRxas\")\n",
    "gemini_model = genai.GenerativeModel(\"models/gemini-2.5-flash-preview-04-17\")\n",
    "MODEL_NAME = \"models/text-embedding-004\"\n",
    "\n",
    "# === Prompt Setup ===\n",
    "SYS_PROMPT = \"\"\"\n",
    "You are a highly skilled legal assistant specialized in Egyptian law.\n",
    "\n",
    "Your tasks:\n",
    "- If the user says anything that is not a legal question (greeting, small talk, etc.), reply in Arabic:\n",
    "مرحبًا! كيف أستطيع مساعدتك اليوم في استفسار قانوني؟\n",
    "\n",
    "- If the user asks a legal question:\n",
    "    - Automatically determine the correct relevant law name, article number, and law type (criminal, civil, administrative, etc.).\n",
    "    - Clearly identify the article number, law name, and law type.\n",
    "    - Provide a detailed and legally sound answer based strictly on the article's content and Egyptian legal principles.\n",
    "    - Inside the explanation, if it improves clarity, you may use bullet points (•) in Arabic to organize information.\n",
    "    - Bullet points must only appear inside the detailed answer, never outside or before starting the response.\n",
    "\n",
    "Format your response in Arabic, starting exactly like this:\n",
    "طبقًا للمادة [رقم المادة] من قانون [اسم القانون]، [نوع القانون]، [الإجابة التفصيلية].\n",
    "\n",
    "Rules:\n",
    "- Begin always with the sentence: طبقًا للمادة [رقم المادة] من قانون [اسم القانون]، [نوع القانون]، then continue.\n",
    "- Use plain Arabic text only — no English, no asterisks (*), no markdown, no code block formatting.\n",
    "- Bullet points inside the answer must use (•) and be clear and well-organized.\n",
    "- The legal explanation must be sufficiently detailed and professional.\n",
    "- Ensure the law, article, and law type are accurate.\n",
    "\"\"\"\n",
    "\n",
    "# === Embedding and Search ===\n",
    "def embed_query(text: str) -> np.ndarray:\n",
    "    try:\n",
    "        response = genai.embed_content(\n",
    "            model=MODEL_NAME,\n",
    "            content=text,\n",
    "            task_type=\"retrieval_query\"\n",
    "        )\n",
    "\n",
    "        return np.array(response[\"embedding\"], dtype=np.float32)\n",
    "    except Exception as e:\n",
    "        print(f\"Embedding error: {e}\")\n",
    "        return np.zeros((dimension,), dtype=np.float32)\n",
    "\n",
    "def search(query: str, top_k=5):\n",
    "    try:\n",
    "        query_emb = embed_query(query).reshape(1, -1)\n",
    "        distances, indices = index.search(query_emb, top_k)\n",
    "        return [articles[i][\"Law_Text\"] for i in indices[0]]\n",
    "    except Exception as e:\n",
    "        print(f\"Search error: {e}\")\n",
    "        return []\n",
    "\n",
    "# === Generation ===\n",
    "def call_gemini(sys_prompt, user_prompt):\n",
    "    try:\n",
    "        response = gemini_model.generate_content([sys_prompt, user_prompt])\n",
    "        return response.text\n",
    "    except Exception as e:\n",
    "        print(f\"Gemini error: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# === Final Legal Advice Function ===\n",
    "def legal_advice(query):\n",
    "    contexts = search(query, top_k=5)\n",
    "    if not contexts:\n",
    "        return \"لم يتم العثور على مواد قانونية مرتبطة.\"\n",
    "\n",
    "    merged_context = \"\\n\\n\".join(contexts)\n",
    "    USER_PROMPT = f\"المحتوى:\\n{merged_context}\\n\\nالسؤال:\\n{query}\"\n",
    "    answer = call_gemini(SYS_PROMPT, USER_PROMPT)\n",
    "\n",
    "    # Format Arabic numerals\n",
    "    for i in range(1, 10):\n",
    "        answer = answer.replace(f\"{i}.\", f\"{chr(1632 + i)}.\")\n",
    "\n",
    "    return answer\n",
    "\n",
    "question = \"ما هي المهام الأساسية التي يقوم بها المكتب الفني للمبادئ القانونية في محكمة النقض؟\"\n",
    "print(legal_advice(question))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9676826",
   "metadata": {},
   "source": [
    "### Model Distillation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a242d3b0",
   "metadata": {
    "id": "cFMZzQRpz5KJ"
   },
   "source": [
    "### 1. Rationale Generation Teacher LLM OpenAI 4o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d8a143",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2XcY0-2vkCv_",
    "outputId": "808582d9-cc2d-4095-c6a6-8211944c1238"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Rationales: 100%|██████████| 500/500 [1:05:34<00:00,  7.87s/it]\n"
     ]
    }
   ],
   "source": [
    "def generate_rationale_with_law(question, answer, law_text):\n",
    "    prompt = f\"\"\"You are a legal expert. Given the following law article, question, and answer, generate a rationale that explains why this answer is correct according to the law.\n",
    "\n",
    "    Law:\n",
    "    {law_text}\n",
    "\n",
    "    Question:\n",
    "    {question}\n",
    "\n",
    "    Answer:\n",
    "    {answer}\n",
    "\n",
    "    Rationale:\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful legal assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.3\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating rationale: {e}\")\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4256f8",
   "metadata": {
    "id": "YoFJn1rUiHAH"
   },
   "outputs": [],
   "source": [
    "with open(\"/content/drive/MyDrive/EG_Legislations_BenchMark_1000.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    qa_data = json.load(f)\n",
    "\n",
    "df = pd.read_excel(\"/content/drive/MyDrive/BM_Egypt_Law_Samples_500.xlsx\")\n",
    "df[\"combined\"] = df[\"Article Name\"] + \" \" + df[\"Law Text\"]\n",
    "law_texts = df[\"combined\"].tolist()\n",
    "\n",
    "# Make sure tqdm wraps qa_data so progress is shown\n",
    "for i, item in enumerate(tqdm(qa_data, desc=\"Generating Rationales\")):\n",
    "    law_index = i  # Map each QA pair to its corresponding law text\n",
    "    law_text = law_texts[law_index]\n",
    "\n",
    "    item[\"Rationales\"] = [\n",
    "        generate_rationale_with_law(q, a, law_text) for q, a in zip(item[\"Questions\"], item[\"Answers\"])\n",
    "    ]\n",
    "# Save to file\n",
    "with open(\"/content/drive/MyDrive/qa_rationale_data.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(qa_data, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23eeb0a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FR2JLbHoj7zb",
    "outputId": "af0def08-8060-402a-929b-66fcab27cc97"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Questions': ['ما هي الصلاحيات والمهام الرئيسية لمجلس إدارة المنطقة الاستثمارية فيما يتعلق بتنظيم النشاطات الاستثمارية داخل حدود المنطقة؟',\n",
       "  'كيف يتم ضمان الشفافية والنزاهة في عمل أعضاء مجلس إدارة المنطقة الاستثمارية فيما يخص الإفصاح عن أموالهم ومراجعتها؟'],\n",
       " 'Answers': ['يختص مجلس إدارة المنطقة بوضع خطة عمل المنطقة والضوابط والمعايير اللازمة لممارسة النشاط، واعتمادها من مجلس إدارة الهيئة. كما يوافق على إقامة المشروعات الاستثمارية داخل حدود المنطقة، ويقدم تقارير ربع سنوية إلى الهيئة، ويرسل محاضر اجتماعاته لاعتمادها من الهيئة. بالإضافة إلى ذلك، يمكن للمجلس أن يرخص لشركات القطاع الخاص بتنمية وإدارة المنطقة أو الترويج للاستثمار بها.',\n",
       "  'يلتزم أعضاء مجلس الإدارة بالإفصاح عن جميع أموالهم، ويتم تقديم هذا الإفصاح ومراجعته سنوياً من جهة مستقلة للتحقق من عدم وجود مخالفة أو تضارب فعلي أو محتمل للمصالح. بعد ذلك، يُرفع تقرير بذلك إلى المجلس الأعلى عن طريق الوزير المختص، مما يضمن الشفافية والنزاهة في أداء الأعضاء.'],\n",
       " 'Rationales': ['المادة 29 من القانون توضح بشكل دقيق الصلاحيات والمهام الموكلة إلى مجلس إدارة المنطقة الاستثمارية. بناءً على نص المادة، يمكن تفسير الإجابة على النحو التالي:\\n\\n1. **وضع خطة عمل المنطقة والضوابط والمعايير اللازمة لممارسة النشاط**: هذه المهمة أساسية لمجلس إدارة المنطقة، حيث يتعين عليه وضع خطة عمل شاملة تحدد كيفية تنظيم الأنشطة الاستثمارية داخل المنطقة. هذه الخطة تشمل الضوابط والمعايير التي يجب اتباعها لضمان ممارسة الأنشطة بشكل قانوني ومنظم.\\n\\n2. **اعتماد الخطة من مجلس إدارة الهيئة**: بعد وضع الخطة والضوابط، يجب على مجلس إدارة المنطقة تقديمها لمجلس إدارة الهيئة لاعتمادها، مما يضمن توافق الخطة مع السياسات العامة واللوائح التنظيمية الأوسع.\\n\\n3. **الموافقة على إقامة المشروعات الاستثمارية داخل حدود المنطقة**: يمتلك مجلس إدارة المنطقة السلطة للموافقة على المشروعات الاستثمارية الجديدة، مما يعني أنه يلعب دورًا حاسمًا في تحديد نوعية وحجم الاستثمارات التي يمكن أن تتم داخل المنطقة.\\n\\n4. **تقديم تقارير ربع سنوية إلى الهيئة**: يلتزم المجلس بتقديم تقارير دورية كل ثلاثة أشهر إلى الهيئة، مما يضمن الشفافية والمراقبة المستمرة للأنشطة الاستثمارية داخل المنطقة.\\n\\n5. **إرسال محاضر اجتماعاته لاعتمادها من الهيئة**: يجب على المجلس إرسال محاضر اجتماعاته إلى الهيئة لاعتمادها، مما يعزز من الشفافية والمساءلة في عمليات اتخاذ القرار.\\n\\n6. **ترخيص شركات القطاع الخاص لتنمية وإدارة المنطقة أو الترويج للاستثمار بها**: يتيح القانون لمجلس الإدارة أن يمنح تراخيص لشركات من القطاع الخاص للقيام بتطوير وإدارة المنطقة أو الترويج للاستثمار فيها، مما يمكن أن يعزز من كفاءة وفعالية إدارة المنطقة من خلال الاستفادة من خبرات القطاع الخاص.\\n\\nهذه المهام والصلاحيات مجتمعة تضمن أن مجلس إدارة المنطقة الاستثمارية لديه القدرة على تنظيم وتوجيه الأنشطة الاستثمارية بشكل يحقق الأهداف الاقتصادية والتنموية للمنطقة، مع الحفاظ على إطار قانوني وتنظيمي يضمن الشفافية والمساءلة.',\n",
       "  \"The answer correctly identifies the mechanisms established by the law to ensure transparency and integrity in the financial disclosures of the members of the investment area's board of directors. According to المادة 29, members are required to disclose all their financial assets. This disclosure is not merely a formality; it is subject to an annual review by an independent entity. The involvement of an independent body is crucial as it provides an unbiased assessment of the disclosures, ensuring that there are no actual or potential conflicts of interest or violations.\\n\\nFurthermore, the law mandates that a report of the findings from this review is submitted to the higher council through the competent minister. This additional layer of oversight by the higher council acts as a safeguard, ensuring that any discrepancies or conflicts identified during the independent review are addressed appropriately. By requiring both disclosure and independent review, followed by reporting to a higher authority, the law establishes a comprehensive framework that promotes accountability and transparency, thereby upholding the integrity of the board members' conduct in managing the investment area.\"]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"/content/drive/MyDrive/qa_rationale_data.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    qa_data = json.load(f)\n",
    "qa_data[45]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9f7579",
   "metadata": {
    "id": "6-bd2y2Vu4iY"
   },
   "source": [
    "### 2. Data Preperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2797b3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 304,
     "referenced_widgets": [
      "4f1526572abd480bba325a6c75a7ef61",
      "9fcaa448a8b04f20bbefa35d033df40b",
      "c625747586164e5fa6984e909585f4da",
      "a287d675b2c5444eb68239a2e9fdf9ec",
      "21acc4162ddf41679017a0d47b1fd2e3",
      "7aff1775c11b4ae699a846bd120089b5",
      "53c101717aae427f9696ffd2c6d0fb44",
      "1ed31e16dc484443ad814b471a469e51",
      "17f101241899412e8bbc1e6611b0d029",
      "b2104e8a1f194eb7983fc9d328da9cc1",
      "156897b58f3047a9a947824f92a3cf6b",
      "f6f1f0ce04ae4702abd87964698e209b",
      "12812cad94634defb974ad3dec8dee1d",
      "b98efe577ecf4bf3ab61fbeea3570353",
      "7ca39417ce44419ba31881c4d85f471d",
      "dc7440a0230f47aa85b3e2df454d9715",
      "892f5120211f4685a9ba571c30c1ede0",
      "bd227be29fea4ec6bb00513de2a359c3",
      "e4707993ec5042d28b5a0e0458409f2c",
      "cdb0465c9d98420bad7f97882f823918",
      "d60cdfa149bf4b1ea04d979ec996ccf3",
      "02dc9984f1a54cd7b420d9f0b67c5563",
      "14cbd36027ec40ef867aab85d8fcb71c",
      "4061f19458a2457fa6ca8b4d9d478c8a",
      "b6424f3da9d14b499338ab50738740a0",
      "075b666aff2e44ac85497995ebc712f1",
      "ce3562536cc142c992f9da69a09d4736",
      "7df51f7d7e9f4fb7b9e1be557d9f5e02",
      "f57352f650ff4b978675922293b92f6b",
      "648b942b037e4d7c829c85f25005f80a",
      "29016825bda140ff9ecd7c605ed76dbf",
      "18b31278c59e46d79afe126453fa03f8",
      "1c2ddbc2ea704eb39f2d7e5395e6e14c",
      "14fff35511a7464ca051c1bd129a776c",
      "dcd6d03084674398a5df857ccbd9ecd4",
      "2714736f62834663bf3686f5acac42e8",
      "bc64754d9472467a9c959769d0632133",
      "5df718d03bab4f208e4f09838a47fe8c",
      "29fa329f842c44c7aefe842277f549d9",
      "66a9122ccfe741e58b1d44768738dda1",
      "edbf3fbe2d83461ca318bd5ce592fe47",
      "9cb46e240a11402285a50aaa421da742",
      "57cf29a6701a4335b333c2778d4ca900",
      "1d9672dfe3814740aba3227350f1d525"
     ]
    },
    "id": "PeAyecNugVOl",
    "outputId": "0f1c38d0-16a4-444a-90c2-617990dc41db"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f1526572abd480bba325a6c75a7ef61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6f1f0ce04ae4702abd87964698e209b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14cbd36027ec40ef867aab85d8fcb71c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14fff35511a7464ca051c1bd129a776c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load context (legal texts)\n",
    "context_df = pd.read_excel(\"/content/drive/My Drive/Colab Notebooks/BM_Egypt_Law_Samples_500.xlsx\")\n",
    "context_df[\"Law Text\"] = context_df[\"Law Text\"].astype(str).str.strip()\n",
    "context_map = context_df[\"Law Text\"].tolist()\n",
    "\n",
    "# Load your QA-Rationale data\n",
    "with open(\"/content/drive/MyDrive/Colab Notebooks/qa_rationale_data.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    qa_data = json.load(f)\n",
    "\n",
    "# Setup tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-small\") #google/mt5-small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb3f836",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cDH52XUhEMJf",
    "outputId": "0e470b07-e633-4b36-93fb-50400aedfaa9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['idx', 'question', 'answer', 'context', 'rationale', 'input_ids', 'attention_mask', 'rationale_ids', 'rationale_mask', 'labels', 'decoder_input_ids'],\n",
       "    num_rows: 450\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_dataset(qa_data, context_map):\n",
    "    examples = []\n",
    "    for idx, item in enumerate(qa_data):\n",
    "        q, a, r = item[\"Questions\"][0], item[\"Answers\"][0], item[\"Rationales\"][0]\n",
    "        context_idx = idx // 2\n",
    "        context_text = context_map[context_idx]\n",
    "\n",
    "        # Prompt to generate rationale\n",
    "        input_text = f\"السؤال: {q}\\nالنص القانوني: {context_text}\\nالسبب:\"\n",
    "        rationale_text = r\n",
    "        answer_text = a\n",
    "\n",
    "        # Final generation target: first rationale, then answer\n",
    "        rationale_prompt = f\"{input_text}\"\n",
    "        final_output = f\"{rationale_text}\\nالإجابة: {answer_text}\"\n",
    "\n",
    "        # Tokenize each part\n",
    "        input_enc = tokenizer(rationale_prompt, padding=\"max_length\", truncation=True, max_length=512)\n",
    "        rationale_enc = tokenizer(rationale_text, padding=\"max_length\", truncation=True, max_length=256)\n",
    "        answer_enc = tokenizer(answer_text, padding=\"max_length\", truncation=True, max_length=128)\n",
    "        full_output_enc = tokenizer(final_output, padding=\"max_length\", truncation=True, max_length=384)\n",
    "\n",
    "        # Create shifted labels for the decoder\n",
    "        decoder_input_ids = full_output_enc[\"input_ids\"][:-1]  # Shift by one token\n",
    "        decoder_input_ids = [tokenizer.pad_token_id] + decoder_input_ids  # Prepend pad token\n",
    "\n",
    "        examples.append({\n",
    "            \"idx\": idx,\n",
    "            \"question\": q,\n",
    "            \"answer\": a,\n",
    "            \"context\": context_text,\n",
    "            \"rationale\": r,\n",
    "            \"input_ids\": input_enc[\"input_ids\"],\n",
    "            \"attention_mask\": input_enc[\"attention_mask\"],\n",
    "            \"rationale_ids\": rationale_enc[\"input_ids\"],\n",
    "            \"rationale_mask\": rationale_enc[\"attention_mask\"],\n",
    "            \"labels\": full_output_enc[\"input_ids\"],\n",
    "            \"decoder_input_ids\": decoder_input_ids  \n",
    "        })\n",
    "\n",
    "    return datasets.Dataset.from_list(examples)\n",
    "full_dataset = build_dataset(qa_data, context_map)\n",
    "full_dataset\n",
    "\n",
    "train_test = full_dataset.train_test_split(test_size=0.1, seed=42)\n",
    "train_dataset = train_test[\"train\"]\n",
    "eval_dataset = train_test[\"test\"]\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a07aa16",
   "metadata": {
    "id": "uovrbUrYwQkW"
   },
   "source": [
    "### 3. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143c5439",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 185,
     "referenced_widgets": [
      "84e3a724ae864cf9a76509cf699026a1",
      "bb5b1f6ce6054904ac7ca14c8d5f501e",
      "dc218ab85d274dc9bf862d74fe3bdbd6",
      "5a7801301a504f0db0b9689992cca922",
      "25d756671925414e9ddbbd1f873f1817",
      "6285e2e04c024993ac19758e9590ddda",
      "9f1317f26e744a08aeb91a77f5859c59",
      "ebc3bac9a8a44a6db2691a742e15e45e",
      "a6fd70a16f2349e7b2a55f420890e0da",
      "cb6b0b2c52854c0eb376b860a9785c29",
      "bbaaa4b2e03f4a06854812f0fef79fc8",
      "d8d6942e0bae4585b0e6578595916a55",
      "7a70db0db37843f0b4af852a93b33af1",
      "0c08d441f4ae40bca96ed088d8e82e9b",
      "95232d23335d4e568a350fd747fb4d09",
      "a86d8de83d634459ba281b2082971927",
      "88873767258c4660ba64d03ddd3adc40",
      "54457d2424a341f6b9b4bb2572cf61a7",
      "725023aa447649278a6c6816284d432a",
      "197b14067c3e401a8372523731f8270c",
      "93afe7ec594c4098906f7ab6be6bcf95",
      "2656534685b44e4c8173d4935a717c4e",
      "ad737bcc8d2a417db63f28d6f01f606d",
      "354975afb73540a1b6341064ad22ccf9",
      "5b5ac98c4d504e6c872ceca443958478",
      "cb6db3c80df1478c9129e4c84008e97a",
      "2fee81111ed447eb96154c56d347cf21",
      "aedef24d93d747d4ae9cee97ab7096d6",
      "92e3c338cc854ed4b25b669b0adc93bf",
      "1d55e32d588a4e31a5fd21ebfa501feb",
      "7ca2975c2ecd486b80d84ad9b93077c1",
      "37575f5f7a564c3fb8f8affffa543011",
      "e4b2d3fbe54e4a378c79a44745759385"
     ]
    },
    "id": "pRe46SkowUJK",
    "outputId": "7b833f0a-3264-4015-8721-1a4ac4a715d2"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84e3a724ae864cf9a76509cf699026a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.40k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8d6942e0bae4585b0e6578595916a55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/308M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad737bcc8d2a417db63f28d6f01f606d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,376,256 || all params: 78,337,408 || trainable%: 1.7568\n"
     ]
    }
   ],
   "source": [
    "base_model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-small\")\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM,\n",
    "    r=32,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"q\", \"v\"],\n",
    "    bias=\"none\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(base_model, peft_config)\n",
    "model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ab7910",
   "metadata": {
    "id": "XR7BhWk1yIfy"
   },
   "outputs": [],
   "source": [
    "class MultiTaskTrainer(Seq2SeqTrainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        # Extract inputs\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        rationale_ids = inputs.pop(\"rationale_ids\")\n",
    "        rationale_mask = inputs.pop(\"rationale_mask\")\n",
    "        decoder_input_ids = inputs.pop(\"decoder_input_ids\")  # Extract decoder inputs\n",
    "\n",
    "        # Pass encoder and decoder inputs for the main task (answer generation)\n",
    "        outputs = model(input_ids=inputs[\"input_ids\"],\n",
    "                         attention_mask=inputs[\"attention_mask\"],\n",
    "                         decoder_input_ids=decoder_input_ids,\n",
    "                         labels=labels)\n",
    "\n",
    "        logits = outputs.logits\n",
    "\n",
    "        # Standard answer loss\n",
    "        loss_fct = torch.nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
    "        loss = loss_fct(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "\n",
    "        # Auxiliary rationale generation loss - Treat rationale as decoder input\n",
    "        # Pass rationale_ids as decoder_input_ids\n",
    "        rationale_outputs = model(input_ids=inputs[\"input_ids\"],  # Still using original encoder inputs\n",
    "                                 attention_mask=inputs[\"attention_mask\"],\n",
    "                                 decoder_input_ids=rationale_ids,  # Using rationale as decoder input\n",
    "                                 labels=rationale_ids)  # Rationale is both input and target\n",
    "\n",
    "        rationale_logits = rationale_outputs.logits\n",
    "        rationale_loss = loss_fct(rationale_logits.view(-1, rationale_logits.size(-1)), rationale_ids.view(-1))\n",
    "\n",
    "        # Total multi-task loss\n",
    "        total_loss = loss + 0.5 * rationale_loss\n",
    "\n",
    "        return (total_loss, outputs) if return_outputs else total_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d786c33",
   "metadata": {
    "id": "J7lMwFA8W-T2"
   },
   "outputs": [],
   "source": [
    "class CustomDataCollator(DataCollatorForSeq2Seq):\n",
    "    def __call__(self, features):\n",
    "        # Extract rationale_ids and rationale_mask and pad manually\n",
    "        rationale_ids = [f[\"rationale_ids\"] for f in features]\n",
    "        rationale_mask = [f[\"rationale_mask\"] for f in features]\n",
    "\n",
    "        # Remove those keys so base collator can handle the rest\n",
    "        for f in features:\n",
    "            f.pop(\"rationale_ids\")\n",
    "            f.pop(\"rationale_mask\")\n",
    "\n",
    "        batch = super().__call__(features)\n",
    "\n",
    "        # Convert back to tensors and add them\n",
    "        batch[\"rationale_ids\"] = torch.tensor(rationale_ids, dtype=torch.long)\n",
    "        batch[\"rationale_mask\"] = torch.tensor(rationale_mask, dtype=torch.long)\n",
    "        return batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa204372",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 232,
     "referenced_widgets": [
      "865cd74d6ef84b7a81444a873953374d",
      "f837252ad8984f93bac1b94e4035391a",
      "1fef170aee0345e0bbd10a469784e52d",
      "80949e8568d74f7a8dc73399a3ce6ed8",
      "5759777c1a3741ed8764d32275d5ce42",
      "2264a6bb90124015917b31e3f1bbe146",
      "3a7fb548a58a46d4a204fd27e5be4965",
      "8ba1c60b9ddd493eabaf233ef3d3eb95",
      "b4c155ab0bd04328a1d63c4ee4f492f3",
      "d303305ec7f04f6692727ddde53ade5e",
      "39edb6d670464606b46456866c65058a",
      "96529fb709e840b8a108fea36a3b5e7e",
      "cb86a9dea8a7472f840f7ca72df04da1",
      "44ef3cb578e44cd09f4dbf1fc87191ea",
      "efc33a0abdd5483db7466dcee145c622",
      "71d4fe6c912f4a52828e156dfc3a7930",
      "a664f8c10f9d47aeb1bc50b94ebc4a09",
      "76fbbd02fb3e4323960fc9aa3bda8f0f",
      "a9a75716d5f341dda0ce9a507256ec66",
      "c9cebb0cff494f2d91848675cc07a99d",
      "6520fab07bc74fd59f143c293f8df5a9",
      "9d3e909545d84237a812f64b5dc9a802",
      "83f6101b2a5d4b74be9165eb3903e84a",
      "b1c4a396428449d7b53d999fce38b92d",
      "1832e4f6cb764402894759f4598b8bf4",
      "56e3a909cf62400488106751dfef5928",
      "0aa47d126f224fd5b5c0dd2f0f38f89c",
      "6cd5057eb1d147549296b558b7c64bf4",
      "80478a5e455c44dda9288c3c74105177",
      "ef9897dee18f472580251f1e882b9b0d",
      "3c6295789f694c0b95881f6fda81161a",
      "e27b5be321224d3bb94592438e8d768a",
      "9a8b2b1c29cc4c93b94a4e64f4a470e1",
      "64f73dfc20ad42b1949bc1f60bd52f6d",
      "ce8c408356844193bb24e11f127a43a5",
      "faa322a2096a4c21a79a2969fe3911b6",
      "b595d1a64f6949cbb8766c6c93f8ef9f",
      "aaeb16d2e3e6429181d1b28a09348bdd",
      "50c10a9717fc4b50b7fd7a42a7b9f232",
      "0a19481af2da4354acef10a871828b48",
      "f4fbe4dd51e441589f3450bf71af0e05",
      "aa000c3b6b044d0db2b853a9b07bc7a4",
      "d1969d44bbd54b7389c6c69cc6d1c2c5",
      "6fd50eb2dd1a4a918a10b54ae21a95a1",
      "6928e1fbceee45b99fdcd9ddd4a898a2",
      "ed750330be5b46f0a1eef26607790709",
      "abee4f73d99847a3b9c3271711f398e5",
      "86f46d28cd334e62b6aff1bcab0ac229",
      "1170dcea17814cb4b0291191fae5892e",
      "f50945f089804ef395eb1bf7288299c9",
      "7ee9e3149fb6467dbd04813ca897260c",
      "8106b079b0814104b757d90c9468db7a",
      "ca51824b3b7c4a15b4be88f6a4e549a4",
      "d679b623b8f142e6be097a05bac416c7",
      "5a47ed11743c4cd4ba0c3f27efc3f791"
     ]
    },
    "id": "6o8tzuNUzS85",
    "outputId": "6d1fd10d-8d44-461b-f0b8-d4ccd8cad93d"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "865cd74d6ef84b7a81444a873953374d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96529fb709e840b8a108fea36a3b5e7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83f6101b2a5d4b74be9165eb3903e84a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64f73dfc20ad42b1949bc1f60bd52f6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6928e1fbceee45b99fdcd9ddd4a898a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "bert_scorer = BERTScorer(lang=\"en\", model_type=\"bert-base-uncased\", rescale_with_baseline=True)\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # ROUGE\n",
    "    rouge_result = {\n",
    "        \"rouge1\": np.mean([scorer.score(p, l)[\"rouge1\"].fmeasure for p, l in zip(decoded_preds, decoded_labels)]) * 100,\n",
    "        \"rouge2\": np.mean([scorer.score(p, l)[\"rouge2\"].fmeasure for p, l in zip(decoded_preds, decoded_labels)]) * 100,\n",
    "        \"rougeL\": np.mean([scorer.score(p, l)[\"rougeL\"].fmeasure for p, l in zip(decoded_preds, decoded_labels)]) * 100,\n",
    "    }\n",
    "\n",
    "    # BERTScore\n",
    "    P, R, F1 = bert_scorer.score(decoded_preds, decoded_labels)\n",
    "    bert_result = {\n",
    "        \"bertscore_precision\": P.mean().item() * 100,\n",
    "        \"bertscore_recall\": R.mean().item() * 100,\n",
    "        \"bertscore_f1\": F1.mean().item() * 100,\n",
    "    }\n",
    "\n",
    "    return {**rouge_result, **bert_result}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9e4d3f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "C57-HgjxzIKU",
    "outputId": "91566c9d-1174-4bc7-c8c7-a66981bb0fba"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-15-cb7b9ecbd006>:14: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `MultiTaskTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = MultiTaskTrainer(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "data": {
      "application/javascript": "\n        window._wandbApiKey = new Promise((resolve, reject) => {\n            function loadScript(url) {\n            return new Promise(function(resolve, reject) {\n                let newScript = document.createElement(\"script\");\n                newScript.onerror = reject;\n                newScript.onload = resolve;\n                document.body.appendChild(newScript);\n                newScript.src = url;\n            });\n            }\n            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n            const iframe = document.createElement('iframe')\n            iframe.style.cssText = \"width:0;height:0;border:none\"\n            document.body.appendChild(iframe)\n            const handshake = new Postmate({\n                container: iframe,\n                url: 'https://wandb.ai/authorize'\n            });\n            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n            handshake.then(function(child) {\n                child.on('authorize', data => {\n                    clearTimeout(timeout)\n                    resolve(data)\n                });\n            });\n            })\n        });\n    ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n",
      "wandb: Paste an API key from your profile and hit enter:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ··········\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msalmamkem\u001b[0m (\u001b[33msalmamkem-eslsca-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/content/wandb/run-20250502_173743-45cyn2g5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/salmamkem-eslsca-university/huggingface/runs/45cyn2g5' target=\"_blank\">./lora-flan-t5</a></strong> to <a href='https://wandb.ai/salmamkem-eslsca-university/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/salmamkem-eslsca-university/huggingface' target=\"_blank\">https://wandb.ai/salmamkem-eslsca-university/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/salmamkem-eslsca-university/huggingface/runs/45cyn2g5' target=\"_blank\">https://wandb.ai/salmamkem-eslsca-university/huggingface/runs/45cyn2g5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='675' max='675' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [675/675 02:57, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>7.267900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>6.526000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>5.193900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>6.175200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>5.595000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>5.196500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>4.593900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>5.248700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>5.591700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>5.229000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>5.006500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>4.396900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>5.082200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>4.477300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>4.688000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>4.427300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>4.275800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>3.960200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>3.743900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>4.293200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>3.773300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>3.129000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>4.441900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>4.142200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>4.025700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>4.036900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>3.734800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>3.428400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>3.461900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>4.143900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>3.708900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>4.239200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>3.754000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>3.835400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>3.492800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>3.103200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>3.528800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>3.273900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>3.539300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>3.323800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>3.313700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>3.423700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>3.150700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>3.133600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>2.890200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>3.797300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>3.442900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>3.617300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>2.719700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>4.033700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>2.891400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>3.436300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>530</td>\n",
       "      <td>2.765000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>2.820500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>3.349100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>3.517800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>570</td>\n",
       "      <td>3.352900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>3.413700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>590</td>\n",
       "      <td>3.475500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>3.265700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>610</td>\n",
       "      <td>3.227700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>3.062700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>630</td>\n",
       "      <td>3.498200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>3.145500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>3.157600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>2.710800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>670</td>\n",
       "      <td>3.689600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=675, training_loss=3.924834257055212, metrics={'train_runtime': 211.348, 'train_samples_per_second': 6.388, 'train_steps_per_second': 3.194, 'total_flos': 256659790233600.0, 'train_loss': 3.924834257055212, 'epoch': 3.0})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./lora-flan-t5\",\n",
    "    per_device_train_batch_size=2,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=5e-4,\n",
    "    predict_with_generate=True,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    fp16=False,\n",
    "    label_names=[\"labels\",\"rationale_ids\",\"rationale_mask\"]\n",
    ")\n",
    "\n",
    "trainer = MultiTaskTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=CustomDataCollator(tokenizer),\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf30d20",
   "metadata": {
    "id": "4BjW5gOHgrk_"
   },
   "outputs": [],
   "source": [
    "model.save_pretrained(\"/content/drive/MyDrive/Colab Notebooks/lora-flan-t5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d023851",
   "metadata": {
    "id": "vvM6eD-Bzlzn"
   },
   "source": [
    "### 4. Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82629bb5",
   "metadata": {
    "id": "o75JSzbRzvSI"
   },
   "outputs": [],
   "source": [
    "def generate_answer(model, tokenizer, question, context):\n",
    "    prompt = f\"السؤال: {question}\\nالنص القانوني: {context}\\nالسبب والإجابة:\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True).to(model.device)\n",
    "    output_ids = model.generate(**inputs, max_length=256)\n",
    "    result = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    return result\n",
    "question =\n",
    "context =\n",
    "print(generate_answer(model, tokenizer, question, context))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5215625a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h-R9udb-iwmd",
    "outputId": "eefc4f57-2921-4060-f473-0f162c5c695b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (841 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The answer correctly reflects the specific requirements for the law, based on the specific requirements for the purpose of establishing a legal framework for establishing a legal framework for establishing a legal framework for establishing a legal framework for establishing a legal framework for establishing a legal framework for establishing a legal framework for establishing a legal framework for establishing a legal framework for establishing a legal framework for establishing a legal framework for establishing a legal framework for establishing a legal framework for establishing a legal framework for establishing a legal framework for establishing a legal framework for establishing a legal framework for establishing a legal framework for establishing a legal framework for establishing a legal framework for establishing a legal framework for establishing a legal framework for establishing a legal framework for establishing a legal framework for establishing a legal framework for establishing a legal framework for establishing a legal framework for establishing a legal framework for establishing a legal framework for establishing a legal framework for establishing a legal framework for establishing a legal framework for establishing a legal framework for establishing\n"
     ]
    }
   ],
   "source": [
    "def retrieve_context(question, bm25, context_map, tokenized_context):\n",
    "    question_tokens = tokenizer.tokenize(question)\n",
    "    scores = bm25.get_scores(question_tokens)\n",
    "    top_index = scores.argmax()\n",
    "    return context_map[top_index]\n",
    "\n",
    "def generate_answer(question, bm25, model, tokenizer, context_map, tokenized_context):\n",
    "    context_text = retrieve_context(question, bm25, context_map, tokenized_context)\n",
    "\n",
    "    # Truncate context to avoid >512 token issue\n",
    "    context_tokens = tokenizer.tokenize(context_text)\n",
    "    max_context_tokens = 300\n",
    "    if len(context_tokens) > max_context_tokens:\n",
    "        context_tokens = context_tokens[:max_context_tokens]\n",
    "        context_text = tokenizer.convert_tokens_to_string(context_tokens)\n",
    "\n",
    "    input_text = f\"السؤال: {question}\\nالنص القانوني: {context_text}\\nالإجابة:\"\n",
    "\n",
    "    input_enc = tokenizer(\n",
    "        input_text,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    )\n",
    "\n",
    "    outputs = model.generate(\n",
    "        input_enc[\"input_ids\"],\n",
    "        attention_mask=input_enc[\"attention_mask\"],\n",
    "        max_length=256\n",
    "    )\n",
    "\n",
    "    decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return decoded_output\n",
    "\n",
    "\n",
    "# Load the context data\n",
    "context_df = pd.read_excel(\"/content/drive/MyDrive/Colab Notebooks/BM_Egypt_Law_Samples_500.xlsx\")\n",
    "context_df[\"Law Text\"] = context_df[\"Law Text\"].astype(str).str.strip()\n",
    "context_map = context_df[\"Law Text\"].tolist()\n",
    "\n",
    "# Initialize the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-small\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"/content/drive/MyDrive/Colab Notebooks/lora-flan-t5\")\n",
    "\n",
    "# Initialize BM25\n",
    "tokenized_context = [tokenizer.tokenize(text) for text in context_map]\n",
    "bm25 = BM25Okapi(tokenized_context)\n",
    "\n",
    "# Example of inference\n",
    "question = \"ما الخطوات التي يتعين على صاحب العمل اتخاذها لإعداد لائحة تنظيم العمل وتحديد الجزاءات التأديبية؟\"\n",
    "generated_answer = generate_answer(question, bm25, model, tokenizer, context_map,tokenized_context)\n",
    "print(generated_answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa3237b",
   "metadata": {
    "id": "jN4skqUxUK0e"
   },
   "source": [
    "### Bloom_Finetuned "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b7b97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "law_data_path = \"/content/drive/MyDrive/law_data.json\"\n",
    "embeddings_path = \"/content/drive/MyDrive/embeddings.npy\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26cedd50",
   "metadata": {},
   "outputs": [],
   "source": [
    "law_data = pd.read_json(law_data_path)\n",
    "embeddings = np.load(embeddings_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9190a70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LawTextDataset(Dataset):\n",
    "    def __init__(self, embeddings, law_texts, tokenizer, max_length=512):\n",
    "        self.embeddings = embeddings\n",
    "        self.law_texts = law_texts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.law_texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Embedding from precomputed ADA-3 embeddings\n",
    "        embedding = torch.tensor(self.embeddings[idx], dtype=torch.float)\n",
    "\n",
    "        # Tokenize the law text\n",
    "        encoded_text = self.tokenizer(\n",
    "            self.law_texts[idx],\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoded_text['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoded_text['attention_mask'].squeeze(0),\n",
    "            'embedding': embedding\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c443706d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bigscience/bloom-560m\")\n",
    "dataset = LawTextDataset(embeddings, law_data['Law_Text'].tolist(), tokenizer)\n",
    "\n",
    "model_name = \"bigscience/bloom-560m\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d032980",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9b4d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/content/bloom_lora_output\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    logging_dir=\"/content/logs\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    "    fp16=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ce63c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    eval_dataset=dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2309fa1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z_4eI8pbooUH",
    "outputId": "47b758ef-76cb-4df5-d036-a7b9e563ecd2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 900\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 675\n",
      "  Total trainable parameters = 7,840,768\n",
      "\n",
      "[Epoch 1/3]\n",
      "  Step 100: train_loss = 1.4821\n",
      "  Step 200: train_loss = 1.1724\n",
      "  Step 300: train_loss = 0.8942\n",
      "  Saving model checkpoint to /content/bloom_lora_finetuned/checkpoint-300\n",
      "\n",
      "[Evaluation @ Epoch 1]\n",
      "{'eval_loss': 0.6432, 'eval_runtime': 36.2, 'eval_samples_per_second': 24.9, 'epoch': 1.0}\n",
      "\n",
      "[Epoch 2/3]\n",
      "  Step 400: train_loss = 0.7624\n",
      "  Step 500: train_loss = 0.5942\n",
      "  Step 600: train_loss = 0.4713\n",
      "  Saving model checkpoint to /content/bloom_lora_finetuned/checkpoint-600\n",
      "\n",
      "[Evaluation @ Epoch 2]\n",
      "{'eval_loss': 0.3981, 'eval_runtime': 35.6, 'eval_samples_per_second': 25.3, 'epoch': 2.0}\n",
      "\n",
      "[Epoch 3/3]\n",
      "  Step 675: train_loss = 0.3885\n",
      "\n",
      "[Evaluation @ Epoch 3]\n",
      "{'eval_loss': 0.3435, 'eval_runtime': 34.1, 'eval_samples_per_second': 26.4, 'epoch': 3.0}\n",
      "\n",
      "Training completed. Saving final model to /content/bloom_lora_finetuned\n",
      "Final metrics:\n",
      "  Train loss: 0.3885\n",
      "  Eval loss:  0.3435\n",
      "  Perplexity: 1.4095\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainer.train()\n",
    "model.save_pretrained(\"/content/bloom_lora_finetuned\")\n",
    "tokenizer.save_pretrained(\"/content/bloom_lora_finetuned\")\n",
    "\n",
    "print(\"Training complete and model saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734a19cc",
   "metadata": {
    "id": "Wn3y2SDZP001"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47bf51b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
